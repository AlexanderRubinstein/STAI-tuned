import argparse
import asyncio
import datetime
import multiprocessing
import os
import copy
import re
import shlex
import signal
import time
import warnings
from tempfile import NamedTemporaryFile
import subprocess
import shutil
import sys
import multiprocessing as mp
from typing import Dict, List
from warnings import warn

import psutil

from stuned.GSheetBatchUpdater import GSheetBatchUpdater
from stuned.job import Job, JobStatus, find_job_id_by_row_id, find_job_idx, get_slurm_job_status
from stuned.job_manager import JobManager
from stuned.utility.local_processing_utils import process_exists

# local modules
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from stuned.utility.utils import (
    DEFAULT_ENV_NAME,
    NEW_SHELL_INIT_COMMAND,
    make_autogenerated_config_name,
    read_yaml,
    save_as_yaml,
    update_dict_by_nested_key,
    get_project_root_path,
    decode_strings_in_dict,
    read_csv_as_dict,
    normalize_path,
    check_duplicates,
    expand_csv,
    retrier_factory,
    is_number
)
from stuned.utility.configs import (
    AUTOGEN_PREFIX,
    NESTED_CONFIG_KEY_SEPARATOR,
    make_csv_config
)
from stuned.utility.logger import (
    STATUS_CSV_COLUMN,
    SUBMITTED_STATUS,
    WHETHER_TO_RUN_COLUMN,
    DELTA_PREFIX,
    SLURM_PREFIX,
    PREFIX_SEPARATOR,
    PLACEHOLDERS_FOR_DEFAULT,
    make_logger,
    make_gdrive_client,
    sync_local_file_with_gdrive,
    log_csv_for_concurrent,
    fetch_csv,
    try_to_upload_csv,
    make_delta_column_name, HTTP_PREFIX, GspreadClient
)
# Arnas' changes
DELTA_AFFECTS_ONLY_FIXED_PARAMS = True
EMPTY_VALUE_MEANS_NO_CHANGE = True

##

FILES_URL = "https://drive.google.com/file"

USE_SRUN = False

OPEN_SOCKET = True

PATH_TO_DEFAULT_CONFIG_COLUMN = "path_to_default_config"
MAIN_PATH_COLUMN = "path_to_main"
DEV_NULL = "/dev/null"
DEFAULT_SLURM_ARGS_DICT = {
    "partition": "gpu-2080ti-beegfs",
    "gpus": 1,
    "time": "02:00:00",
    "ntasks": 1,
    "cpus-per-task": 2,
    "error": DEV_NULL,
    "output": DEV_NULL
}
EMPTY_STRING = "EMPTY_STRING"
EXPANDED_CSV_PREFIX = "expanded_"
CURRENT_ROW_PLACEHOLDER = "__ROW__"
CURRENT_WORKSHEET_PLACEHOLDER = "__WORKSHEET__"
MAX_PROCESSES = 16

# Monitor-related constants
MONITOR_STATUS_COLUMN = "status_monitor"
MONITOR_EXIT_CODE_COLUMN = "exit_code_monitor"
MONITOR_JOB_ID_COLUMN = "slurm_job_id_monitor"
MONITOR_LAST_UPDATE_COLUMN = "last_update_monitor"

# use_shared_memory = sys.version_info >= (3, 8, 3)
# if use_shared_memory:
#     pass

def parse_args():
    parser = argparse.ArgumentParser(
        description="Train and/or validate models."
    )
    parser.add_argument(
        "--csv_path",
        type=str,
        required=True,
        help="path to csv file"
    )
    parser.add_argument(
        "--conda_env",
        type=str,
        required=False,
        default=DEFAULT_ENV_NAME,
        help="conda environment name"
    )
    parser.add_argument(
        "--run_locally",
        action="store_true",
        help="whether to run this script locally"
    )
    parser.add_argument(
        "--log_file_path",
        type=str,
        required=False,
        default=get_default_log_file_path(),
        help="default path for the log file"
    )
    parser.add_argument(
        "--expand",
        action="store_true",
        help="whether to first expand input csv by cartesian product of options"
    )

    parser.add_argument(
        "--use_socket",
        action="store_true",
        help="whether to use a socket for communication with the server"
    )
    return parser.parse_args()


def get_default_log_file_path():
    return os.path.join(
        get_project_root_path(),
        "tmp",
        "tmp_log_for_run_from_csv.out"
    )


#
# def main(make_final_cmd=None, allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX)):
#     """ Like main() but adds continous monitoring of the jobs """
#     if make_final_cmd is None:
#         make_final_cmd = make_final_cmd_slurm
#
#     args = parse_args()
#
#     logger = make_logger()
#
#     csv_path_or_url = args.csv_path
#
#     logger.log(f"Fetching csv from: {csv_path_or_url}")
#     csv_path, spreadsheet_url, worksheet_name, gspread_client = fetch_csv(
#         csv_path_or_url,
#         logger
#     )
#
#     if args.expand:
#         expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client)
#
#     inputs_csv = read_csv_as_dict(csv_path)
#
#     with mp.Manager() as shared_memory_manager:
#
#         lock = shared_memory_manager.Lock()
#         current_step = shared_memory_manager.Value("int", 0)
#         shared_rows_to_run = shared_memory_manager.list()
#         shared_csv_updates = shared_memory_manager.list()
#         shared_row_numbers = shared_memory_manager.list()
#         shared_default_config_paths = shared_memory_manager.dict()
#
#         starmap_args_for_row_processing = [
#             (
#                 make_final_cmd,
#                 csv_row,
#                 row_number,
#                 csv_path,
#                 args.conda_env,
#                 args.run_locally,
#                 args.log_file_path,
#                 spreadsheet_url,
#                 worksheet_name,
#                 logger,
#                 lock,
#                 shared_rows_to_run,
#                 shared_default_config_paths,
#                 shared_csv_updates,
#                 shared_row_numbers,
#                 current_step,D
#                 len(inputs_csv)
#             )
#                 for row_number, csv_row in inputs_csv.items()
#         ]
#
#         if len(starmap_args_for_row_processing):
#
#             first_csv_row = starmap_args_for_row_processing[0][1]
#             check_csv_column_names(first_csv_row, allowed_prefixes)
#
#             pool_size = get_pool_size(len(starmap_args_for_row_processing))
#
#             upload_csv = False
#
#             with mp.Pool(pool_size) as pool:
#
#                 pool.starmap(
#                     process_csv_row,
#                     starmap_args_for_row_processing
#                 )
#
#                 if len(shared_rows_to_run):
#
#                     assert 2 * len(shared_rows_to_run) == len(shared_csv_updates)
#
#                     concurrent_log_func = retrier_factory()(log_csv_for_concurrent)
#
#                     concurrent_log_func(
#                         csv_path,
#                         shared_csv_updates
#                     )
#
#                     os.makedirs(os.path.dirname(args.log_file_path), exist_ok=True)
#
#                     starmap_args_for_job_submitting = [
#                         (
#                             run_cmd,
#                             args.log_file_path
#                         )
#                             for run_cmd in shared_rows_to_run
#                     ]
#
#                     pool.starmap(
#                         submit_job,
#                         starmap_args_for_job_submitting
#                     )
#                     upload_csv = True
#
#             if upload_csv:
#                 try_to_upload_csv(
#                     csv_path,
#                     spreadsheet_url,
#                     worksheet_name,
#                     gspread_client
#                 )

def get_all_slurm_jobs():
    try:
        output = subprocess.check_output(['squeue', '--me'], universal_newlines=True, timeout=60)
        lines = output.strip().split('\n')[1:]  # Skip the header
        jobs = {}
        for line in lines:
            parts = line.split()
            job_id = int(parts[0])
            status = parts[4]
            jobs[job_id] = status
        return jobs
    except subprocess.CalledProcessError:
        return {}
    except subprocess.TimeoutExpired:
        return None
def get_pid_job_stats(jobs_ids, logger):
    job_stats_pid = {}
    for job_id in jobs_ids:
        # Just check if the process exists
        if process_exists(job_id):
            job_stats_pid[job_id] = JobStatus.RUNNING
        else:
            job_stats_pid[job_id] = JobStatus.FAILED # I guess?
    return job_stats_pid


def get_slurm_jobs_stats(jobs_ids, logger):
    # Convert the list of job IDs to a comma-separated string
    job_ids_str = ','.join(map(str, jobs_ids))

    # Define the sacct command with the desired format
    cmd = [
        'sacct',
        '-j', job_ids_str,
        '--format=JobID,State,ExitCode',
        '--noheader'
    ]

    # Execute the command and capture the output
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
    except subprocess.TimeoutExpired:
        logger.log("Warning: sacct command timed out! Will repeat after sleeping")
        return None
    # Split the output into lines and then into columns
    lines = result.stdout.strip().split('\n')
    job_stats = {}
    for line in lines:
        columns = line.split()
        # Check if the line contains the main job info (not the steps)
        if len(columns) == 3 and '+' not in columns[0]:
            job_id, state, exit_code = columns
            job_stats[int(job_id)] = {
                'State': state,
                'ExitCode': exit_code
            }

    return job_stats

def update_job_statuses_in_place(job_manager : JobManager, shared_jobs_copy : Dict, logger, local_run=False) -> bool:
    # Check SLURM cluster for running jobs and their status
    ref_key_job_id = "main_pid" if local_run else "job_id"
    job_ids = [job[ref_key_job_id] for job in shared_jobs_copy.values()]

    if not local_run:
        # Check SLURM cluster for jobs and their status
        all_jobs_stats = get_slurm_jobs_stats(job_ids, logger)
        if all_jobs_stats is None:
            return False

        if all_jobs_stats == {}:
            return True
    else:
        # Status should be updated in the shared_jobs_copy itself
        pass

    # updated_rows = []
    for row_id, job in shared_jobs_copy.items():
        if not local_run:
            job_stat = all_jobs_stats.get(job[ref_key_job_id], None)
            if job_stat:
                slurm_status = job_stat["State"]
                exit_code = job_stat["ExitCode"]
            else:
                slurm_status = None
                exit_code = None

            job_status = get_slurm_job_status(slurm_status)
        else:
            job_status = job["status"] if "status" in job else None
            exit_code = job["exit_code"] if "exit_code" in job else None
            
        job_from_local_idx = find_job_idx(job_manager.jobs, job[ref_key_job_id])
        if job_from_local_idx is None:
            # Add a job to the list and mark as update
            job_to_add = Job(job_id = job[ref_key_job_id], job_status = job_status, job_exit_code = exit_code, csv_row_id = row_id)
            job_to_add.updated = True
            job_manager.jobs.append(job_to_add)
            # updated_rows.append(row_id)
        else:
            job_from_local = job_manager.jobs[job_from_local_idx]
            if job_from_local.csv_row_id is None:
                job_from_local.csv_row_id = row_id
            if job_from_local.job_status != job_status or job_from_local.job_exit_code != exit_code:
                # Update the job and mark as update
                job_from_local.job_status = job_status
                job_from_local.job_exit_code = exit_code
                job_from_local.updated = True
                # updated_rows.append(row_id)
            job_manager.jobs[job_from_local_idx] = job_from_local
    return True

def monitor_jobs_async(job_manager : JobManager, async_results, shared_jobs_dict, run_locally: bool, logger, spreadsheet_url, worksheet_name: str,
                       shared_row_numbers, csv_path, gsheet_client: GspreadClient, lock_manager, n_jobs_total):
    # Prepare google client for writing the updates. The mechanism of writing the updates here is different
    # from the individual jobs. Here we write the updates "globally" to the worksheet as opposed to a local csv file.
    # spreadsheet_from_url = logger.get_spreadsheet_by_url(get_spreadsheet_by_urll)

    # Define a flag to check if we should exit
    should_exit = False

    # Signal handler function
    def signal_handler(sig, frame):
        nonlocal should_exit
        run_jobs_flag.value = 0
        logger.log("Received exit signal. Preparing to terminate all jobs...")
        should_exit = True

    # Register the signal handler
    signal.signal(signal.SIGINT, signal_handler)

    gsheet_updater = None
    shared_row_numbers_lst = list(shared_row_numbers)

    if csv_path is None:
        assert run_locally, "If not running locally, a csv file must be provided."
        warn("No csv file provided, skipping writing to a csv file.")
    else:
        gsheet_updater = GSheetBatchUpdater(spreadsheet_url, worksheet_name, gsheet_client, logger, csv_path)

        # Prepare the updates
        "status	walltime	starttime	endtime	WandB url	stdout	stderr"
        # TODO: make this more robust
        reset_columns = ["status", "walltime", "starttime", "endtime", "WandB url", "stdout", "stderr", "slurm_job_id_monitor",
                         "gpu_info", "cpu_count", "exit_code_monitor", "last_update_monitor"]
        column_value_pairs = [
            (MONITOR_LAST_UPDATE_COLUMN, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
            (MONITOR_STATUS_COLUMN, SUBMITTED_STATUS),
        ]
        for column_name in reset_columns:
            column_value_pairs.append((column_name, ""))
        
        # Add updates to the queue
        for row_id_share in shared_row_numbers_lst:
            for key, val in column_value_pairs:
                gsheet_updater.add_to_queue(row_id_share, key, val)

        # Write updates to CSV and then batch update the Google Sheet
        update_status = gsheet_updater.batch_update()
        logger.log(f"Update status to GSheets: {update_status}")

    total_jobs = n_jobs_total
    if total_jobs == 0:
        logger.log("No jobs submitted, exiting.")
        return

    current_sleep_duration = 1  # Start with 3 seconds
    sleep_increment = 1  # Increment by 3 seconds each time
    max_sleep_duration = 3  # Maximum sleep duration

    # local_jobs_dict = {}
    
    last_full_check_time = time.time()
    check_for_full_updates_every = 60
    
    while not should_exit:
        with lock_manager:
            shared_jobs_copy = dict(shared_jobs_dict)
        with job_manager.manager_lock:
            ''' TODO: this can introduce some clashes between status changes; `sacct` should only be used in
                case the job isn't reachable. Otherwise the stauts should be updated from the job itself. '''
            jobs_update_status = update_job_statuses_in_place(job_manager, shared_jobs_copy, logger, run_locally)

            if not jobs_update_status:
                logger.log("Warning: sacct command timed out! Will repeat after sleeping")
                time.sleep(current_sleep_duration)
                continue

            # For each updated row, add the necessary updates to the gsheet_updater queue
            dump_into_gsheet_queue(gsheet_updater, job_manager)

        # Count the statuses
        submitted = submitted_jobs.value
        # running = sum(1 for job in jobs_to_monitor if job.job_status == "Running")
        running = sum(1 for job in job_manager.jobs if job.job_status == JobStatus.RUNNING)
        # finished_successfully = sum(1 for job in shared_jobs_dict.values() if job["status"] == "Completed")
        finished_successfully = sum(1 for job in job_manager.jobs if job.job_status == JobStatus.COMPLETED)
        failed = sum(1 for job in job_manager.jobs if job.job_status == JobStatus.FAILED or job.job_status == JobStatus.CANCELLED)

        # Display progress and status
        last_update_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        status_msg = (f"Jobs Progress | Submitted: {submitted} / {total_jobs} | "
                      f"Running: {running} / {total_jobs} | "
                      f"Finished: {finished_successfully} / {total_jobs} | "
                      f"Failed: {failed} | Last Update: {last_update_time}")
        logger.log(status_msg, carriage_return=True)

        # Periodically report back to Gsheets to indicate the status of the jobs.
        if gsheet_updater is not None:
            gsheet_updater.batch_update()
            
            if not job_manager.open_socket and (time.time() - last_full_check_time > check_for_full_updates_every):
                gsheet_updater.update_remote_if_changes_happened()

        time.sleep(current_sleep_duration)

        # Increment the sleep duration for the next iteration, but don't exceed the maximum
        current_sleep_duration = min(current_sleep_duration + sleep_increment, max_sleep_duration)

        if finished_successfully + failed == n_jobs_total:
            logger.log("All jobs finished.")
            break

    if should_exit:
        running_job_ids = [job.job_id for job in job_manager.jobs if job.job_status in JobStatus.PENDING or job.job_status in JobStatus.RUNNING or (job.job_status is not None and job.job_status.startswith(JobStatus.UNKNOWN + "_"))]
        logger.log(f"Number of jobs that were running: {len(running_job_ids)}")
        exited_jobs = 0
        # Use scancel to send a termination request to these jobs
        for job_id in running_job_ids:
            if run_locally:
                # TODO without SIGKILL
                if process_exists(job_id):
                    os.kill(job_id, signal.SIGKILL)
                else:
                    exited_jobs += 1
            else:
                subprocess.run(['scancel', str(job_id)])

        grace_period = 300
        # TODO: update these things in the manager itself, not outside it
        for second in range(1, grace_period + 1):
            with lock_manager:
                shared_jobs_copy = dict(shared_jobs_dict)

            update_job_statuses_in_place(job_manager, shared_jobs_copy, logger, run_locally)
            still_running = 0
            for job_id in running_job_ids:
                job_idx = find_job_idx(job_manager.jobs, job_id)
                job = job_manager.jobs[job_idx]
                if job.job_status == JobStatus.RUNNING:
                    still_running += 1

            logger.log(
                f"{second}/{grace_period} seconds trying, {len(running_job_ids) - still_running}/{len(running_job_ids)} jobs exited")

            # Break the loop if all jobs have exited
            if still_running == 0:
                logger.log("All jobs terminated.")
                break

            time.sleep(3)  # Sleep for 3 seconds

        if still_running > 0:
            logger.log(f"Warning: {still_running} jobs did not terminate after the grace period.")
        failed += len(running_job_ids)

    # Print summary of jobs' status
    logger.log(f"Number of jobs submitted: {submitted}")
    logger.log(f"Number of jobs finished successfully: {finished_successfully}")
    logger.log(f"Number of jobs failed: {failed}")

    # Make sure to push the updates, if there were any
    if gsheet_updater is not None:
        dump_into_gsheet_queue(gsheet_updater, job_manager)

        gsheet_updater.batch_update(force=True)

    # stop the server
    if job_manager.open_socket:
        logger.log("Stopping the server...")
        exit_code = job_manager.server_process.terminate()
        logger.log(f"Server stopped with exit code {exit_code}")
    sys.exit(0)


def dump_into_gsheet_queue(gsheet_updater, job_manager):
    for idx, job in enumerate(job_manager.jobs):
        if job.updated:
            gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_STATUS_COLUMN, job.job_status)
            gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_LAST_UPDATE_COLUMN,
                                        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
            if job.job_exit_code is not None:
                gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_EXIT_CODE_COLUMN,
                                            f"{job.job_exit_code}")
            if job.job_id is not None:
                gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_JOB_ID_COLUMN, f"{job.job_id}")

            # check the queue of each job too
            for q_item in job.writing_queue:
                gsheet_updater.add_to_queue(job.csv_row_id, q_item[0], q_item[1])
            job_manager.jobs[idx].writing_queue = []
            job_manager.jobs[idx].updated = False


# def monitor_jobs_async_old(async_results, shared_memory_names_or_pids, run_locally : bool, logger, spreadsheet_url, worksheet_name : str,
#                        shared_row_numbers, csv_path, gsheet_client: GspreadClient):
#     # Prepare google client for writing the updates. The mechanism of writing the updates here is different
#     # from the individual jobs. Here we write the updates "globally" to the worksheet as opposed to a local csv file.
#     # spreadsheet_from_url = logger.get_spreadsheet_by_url(get_spreadsheet_by_urll)
#
#     # Define a flag to check if we should exit
#     should_exit = False
#
#     # Signal handler function
#     def signal_handler(sig, frame):
#         nonlocal should_exit
#         run_jobs_flag.value = 0
#         logger.log("Received exit signal. Preparing to terminate all jobs...")
#         should_exit = True
#
#     # Register the signal handler
#     signal.signal(signal.SIGINT, signal_handler)
#     # if spreadsheet_url is None:
#     #     assert run_locally, "If not running locally, a spreadsheet url must be provided."
#     #     warn("No spreadsheet url provided, skipping writing to A spreadsheet.")
#     # else:
#     #     gsheet_client = make_gspread_client(logger)
#     #     remote_spreadsheet = gsheet_client.get_spreadsheet_by_url(spreadsheet_url)
#     #     worksheet = remote_spreadsheet.worksheet(worksheet_name)
#     #     # We don't have to find the column here, more efficient would be to do it based on the local csv file.
#     #     # But for now we'll just do it this way to make sure it works
#     #     column_num_status = get_or_create_column(worksheet, MONITOR_STATUS_COLUMN)
#     #     column_num_last_update_monitor = get_or_create_column(worksheet, MONITOR_LAST_UPDATE_COLUMN)
#     #
#     #     updater = GSheetBatchUpdater(worksheet)
#     #     # Update the status and last update columns in the beginning
#     #     update_gsheet(updater, column_num_status, column_num_last_update_monitor, shared_row_numbers)
#     gsheet_updater = None
#     if csv_path is None:
#         assert run_locally, "If not running locally, a csv file must be provided."
#         warn("No csv file provided, skipping writing to a csv file.")
#     else:
#         # Still need to init the client etc
#         # The writing here must not be attached to a single row. Thus we define our own csv update mechanism.
#
#         # try_to_log_in_csv_in_batch(
#         #     logger,
#         #     [
#         #         (WHETHER_TO_RUN_COLUMN, "100"),
#         #     ]
#         # )
#
#         gsheet_updater = GSheetBatchUpdater(spreadsheet_url, worksheet_name, gsheet_client, logger, csv_path)
#
#         # Prepare the updates
#         shared_row_numbers_lst = list(shared_row_numbers)
#         column_value_pairs = [
#             (MONITOR_LAST_UPDATE_COLUMN, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
#             (MONITOR_STATUS_COLUMN, SUBMITTED_STATUS),
#         ]
#
#         # Add updates to the queue
#         for row_id_share in shared_row_numbers_lst:
#             for key, val in column_value_pairs:
#                 gsheet_updater.add_to_queue(row_id_share, key, val)
#
#         # Write updates to CSV and then batch update the Google Sheet
#         update_status = gsheet_updater.batch_update()
#         logger.log(f"Update status to GSheets: {update_status}")
#         # gsheet_updater = GSheetBatchUpdater()
#         # shared_row_numbers_lst = list(shared_row_numbers)
#         # column_value_pairs = [
#         #     (MONITOR_LAST_UPDATE_COLUMN, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
#         #     (MONITOR_STATUS_COLUMN, SUBMITTED_STATUS),
#         # ]
#         # report_stuff_ids = []
#         # for row_id_share, (key, val) in zip(shared_row_numbers_lst, column_value_pairs):
#         #     report_stuff_ids.append((row_id_share, key, val))
#         #
#         # retrier_factory(logger)(log_csv_for_concurrent)(
#         #     csv_path,
#         #     report_stuff_ids
#         # )
#         # # [
#         # #     # (idx + 1, column_name, value)
#         # #     #     for idx, (column_name, value) in enumerate(column_value_pairs)
#         # #     (row_id, column_name, value) for (row_id, column_name, value) in
#         # #     zip(shared_row_numbers, [column_name for column_name, _ in column_value_pairs],
#         # #         [value for _, value in column_value_pairs])
#         # # ]
#         #
#         # gsheet_client.upload_csvs_to_spreadsheet(spreadsheet_url, [csv_path], [worksheet_name], single_rows_per_csv=[[0]])
#         # gsheet_client.upload_csvs_to_spreadsheet(spreadsheet_url, [csv_path], [worksheet_name], single_rows_per_csv=[shared_row_numbers_lst])
#
#     total_jobs = len(async_results)
#     if total_jobs == 0:
#         logger.log("No jobs submitted, exiting.")
#         return
#
#     job_status = ["submitted" for _ in range(total_jobs)]
#     # To maintain info like the exit code
#     job_info = [None for _ in range(total_jobs)]
#
#     current_sleep_duration = 1  # Start with 3 seconds
#     sleep_increment = 1  # Increment by 3 seconds each time
#     max_sleep_duration = 30  # Maximum sleep duration
#
#     while not should_exit:
#         for idx, result in enumerate(async_results):
#             status_change = False
#             # this logic is only right for local runs
#             if run_locally:
#                 if not result.ready() and job_status[idx] == "submitted":
#                     job_status[idx] = "running"
#                     status_change = True
#                 elif result.ready():
#                     exit_code = result.get()  # Assuming submit_job returns the exit code
#                     job_info[idx] = exit_code  # Store the exit code
#                     if exit_code == 0:
#                         job_status[idx] = "finished_successfully"
#                         status_change = True
#                     else:
#                         job_status[idx] = "failed"
#                         job_info[idx] = f"Job failed with exit code {exit_code}"
#                         status_change = True
#             else:
#                 # TODO: add support for SLURM
#                 raise NotImplementedError("Monitoring for SLURM is not implemented yet.")
#             if status_change:
#                 # Update the status and last update columns in the beginning
#                 if gsheet_updater is not None:
#                     gsheet_updater.add_to_queue(shared_row_numbers_lst[idx], MONITOR_STATUS_COLUMN, job_status[idx])
#                     if job_info[idx] is not None:
#                         gsheet_updater.add_to_queue(shared_row_numbers_lst[idx], MONITOR_EXIT_CODE_COLUMN,
#                                                     job_info[idx])
#
#         # Count the statuses
#         submitted = submitted_jobs.value
#         running = job_status.count("running")
#         finished_successfully = job_status.count("finished_successfully")
#         failed = job_status.count("failed")
#
#         # Display progress and status
#         last_update_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#
#         status_msg = (f"Jobs Progress | Submitted: {submitted} / {total_jobs} | "
#                       f"Finished: {finished_successfully} / {total_jobs} | "
#                       f"Failed: {failed} | Last Update: {last_update_time}")
#
#         logger.log(status_msg, carriage_return=True)
#
#         # Sleep for the current duration
#         time.sleep(current_sleep_duration)
#
#         # Increment the sleep duration for the next iteration, but don't exceed the maximum
#         current_sleep_duration = min(current_sleep_duration + sleep_increment, max_sleep_duration)
#
#         # Periodically we want to report back to Gsheets to indicate status of the jobs.
#         if gsheet_updater is not None:
#             gsheet_updater.batch_update()
#
#         if running == 0 and finished_successfully + failed == submitted:
#             logger.log("All jobs finished.")
#             break
#
#     # If we're exiting due to a signal, terminate the jobs
#     exited_jobs = 0
#     if should_exit:
#         running_jobs = sum(1 for result in async_results if not result.ready())
#         logger.log(f"Number of jobs that were running: {running_jobs}")
#
#         # Make sure to update GSheets
#
#         # Step 1: Try to terminate all processes gracefully
#         # for idx, result in enumerate(async_results):
#         #     if run_locally:
#         #         if not result.ready():
#         #             try:
#         #                 os.kill(shared_memory_names_or_pids[idx]["main_pid"], signal.SIGTERM)
#         #             except ProcessLookupError:
#         #                 # PID not found, assume killed
#         #                 job_status[idx] = "failed"
#         #     else:
#         #         # TODO: add job killing for SLURM jobs
#         #         raise NotImplementedError("Killing SLURM jobs is not implemented yet.")
#         # Step 2: Wait for a short duration with periodic checks
#         grace_period = 5
#
#         for second in range(1, grace_period + 1):
#             for idx, result in enumerate(async_results):
#                 if job_status[idx] == "running":
#                     status_change = False
#
#                     if run_locally:
#                         if result.ready():
#                             exited_jobs += 1
#                             status_change = True
#                             job_status[idx] = "failed"
#                             job_info[idx] = "Job gracefully terminated by user"
#                         elif not process_exists(shared_memory_names_or_pids[idx]["main_pid"]):
#                             job_status[idx] = "failed"
#                             job_info[idx] = "Job gracefully terminated by user"
#                             exited_jobs += 1
#                             status_change = True
#                         # elif second == grace_period:  # If we're on the last iteration and the job hasn't exited
#                         #     os.kill(shared_memory_names_or_pids[idx]["main_pid"], signal.SIGKILL)
#                         #     exited_jobs += 1
#                         #     job_status[idx] = "failed"
#                         #     job_info[idx] = "Job forcefully terminated by user"
#                         #     status_change = True
#                     else:
#                         # TODO: add job killing of SLURM jobs
#                         raise NotImplementedError("Killing SLURM jobs is not implemented yet.")
#
#                     if status_change:
#                         if gsheet_updater is not None:
#                             gsheet_updater.add_to_queue(shared_row_numbers_lst[idx], MONITOR_STATUS_COLUMN, job_status[idx])
#                             if job_info[idx] is not None:
#                                 gsheet_updater.add_to_queue(shared_row_numbers_lst[idx], MONITOR_EXIT_CODE_COLUMN,
#                                                             job_info[idx])
#             logger.log(f"{second}/{grace_period} seconds trying, {exited_jobs}/{running_jobs} jobs exited")
#
#             # Break the loop if all processes have exited
#             if exited_jobs == running_jobs:
#                 logger.log("All jobs terminated.")
#                 break
#
#             time.sleep(1)  # Sleep for 1 second
#
#     failed += exited_jobs
#
#     # Print summary of jobs' status
#     logger.log(f"Number of jobs submitted: {submitted}")
#     logger.log(f"Number of jobs finished successfully: {finished_successfully}")
#     logger.log(f"Number of jobs failed: {failed}")
#
#     # Make sure to push the updates, if there were any
#     if gsheet_updater is not None:
#         gsheet_updater.batch_update(force=True)
#
#     # Update the status of all the jobs in the remote sheet
#     # shared_row_numbers_lst = list(shared_row_numbers)
#     # column_value_pairs = [
#     #     (MONITOR_LAST_UPDATE_COLUMN, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
#     #     (MONITOR_STATUS_COLUMN, SUBMITTED_STATUS),
#     # ]
#     # report_stuff_ids = []
#     # for row_id_share, result in zip(shared_row_numbers_lst, column_value_pairs):
#     #     report_stuff_ids.append((row_id_share, result[0], result[1]))
#     #
#     # retrier_factory(logger)(log_csv_for_concurrent)(
#     #     csv_path,
#     #     report_stuff_ids
#     # )
#     # # [
#     # #     # (idx + 1, column_name, value)
#     # #     #     for idx, (column_name, value) in enumerate(column_value_pairs)
#     # #     (row_id, column_name, value) for (row_id, column_name, value) in
#     # #     zip(shared_row_numbers, [column_name for column_name, _ in column_value_pairs],
#     # #         [value for _, value in column_value_pairs])
#     # # ]
#     #
#     # gsheet_client.upload_csvs_to_spreadsheet(spreadsheet_url, [csv_path], [worksheet_name], single_rows_per_csv=[[0]])
#     # gsheet_client.upload_csvs_to_spreadsheet(spreadsheet_url, [csv_path], [worksheet_name],
#     #                                          single_rows_per_csv=[shared_row_numbers_lst])
#     #


#
#
# def monitor_jobs_async(async_results, shared_memory_names_or_pids, run_locally, logger):
#     total_jobs = len(shared_memory_names_or_pids)
#     if total_jobs == 0:
#         return
#
#     job_status = ["not_started" for _ in range(total_jobs)]
#
#     while True:
#         for idx, result in enumerate(async_results):
#             if not result.ready() and job_status[idx] == "not_started":
#                 job_status[idx] = "running"
#             elif result.ready():
#                 exit_code = result.get()  # Assuming submit_job returns the exit code
#                 if exit_code == 0:
#                     job_status[idx] = "finished_successfully"
#                 else:
#                     job_status[idx] = f"finished_with_error_{exit_code}"
#
#         not_started = sum(1 for status in job_status if status == "not_started")
#         running = sum(1 for status in job_status if status == "running")
#         finished = total_jobs - not_started - running
#
#         # Display progress bar for completed jobs
#         logger.progress(
#             "Jobs Completion Progress",
#             finished,
#             total_jobs
#         )
#
#         # Display number of jobs not started and running
#         logger.log(f"Number of jobs not started: {not_started}")
#         logger.log(f"Number of running jobs: {running}")
#
#         if running == 0 and not_started == 0:
#             break
#
#         time.sleep(30)  # Check every 30 seconds
#
#
# def monitor_jobs(shared_memory_names_or_pids, run_locally, logger):
#     """ Monitor the jobs. Depending on the Python version, `shared_memory_names_or_pids` corresponds to shared memory names or PIDs """
#
#     if use_shared_memory:
#         # Use shared_memory_name as the key if it exists, otherwise default to main_pid
#         job_status = {job.get('shared_memory_name', job['main_pid']): "running" for job in shared_memory_names_or_pids}
#     else:
#         # Use main_pid as the key
#         job_status = {job['main_pid']: "running" for job in shared_memory_names_or_pids}
#     total_jobs = len(shared_memory_names_or_pids)
#
#     while True:
#         for key in list(job_status.keys()):
#             if job_status[key] == "running":
#                 if run_locally:
#                     if use_shared_memory:
#                         # Access the shared memory and check the status
#                         ex_shm = shared_memory.SharedMemory(name=key)
#                         dtype = np.dtype([('status', np.int32), ('exit_code', np.int32)])
#                         status_array = np.ndarray(shape=(1,), dtype=dtype, buffer=ex_shm.buf)
#
#                         if status_array[0]['status'] == 0:  # 0 indicates "finished"
#                             job_status[key] = "finished"
#                             print(f"Job with key {key} finished with exit code: {status_array[0]['exit_code']}")
#                             ex_shm.close()
#                             ex_shm.unlink()
#
#                     else:
#                         # Previous method using PIDs
#                         try:
#                             os.kill(key, 0)  # This will not kill the process, just check if it's alive
#                         except ProcessLookupError:
#                             # Process is no longer running
#                             job_status[key] = "finished"
#                         else:
#                             all_finished = False
#
#                 else:
#                     # SLURM logic to check job status
#                     # You might use `squeue` or other SLURM commands to check job status
#                     # For simplicity, I'm just marking them as finished
#                     job_status[key] = "finished"  # Replace this with actual SLURM logic
#
#         running = sum(1 for status in job_status.values() if status == "running")
#         finished = sum(1 for status in job_status.values() if status == "finished")
#         failed = total_jobs - running - finished
#
#         # Display progress bar for completed jobs
#         logger.progress(
#             "Jobs Completion Progress",
#             finished,
#             total_jobs
#         )
#
#         # Display number of failed jobs
#         if failed > 0:
#             logger.log(f"Number of failed jobs: {failed}")
#
#         if running == 0:
#             break
#
#         time.sleep(30)  # Check every 30 seconds

# def monitor_jobs(shared_job_ids, run_locally, logger):
#     """ Monitor the jobs. If SLURM, `shared_job_ids` corresponds to job IDs, otherwise to PIDs """
#
#     job_status = {job_id: "running" for job_id in shared_job_ids}
#     total_jobs = len(shared_job_ids)
#
#     while True:
#         for job_id in list(job_status.keys()):
#             if job_status[job_id] == "running":
#                 if run_locally:
#                     # Check if the process is still running
#                     try:
#                         os.kill(job_id, 0)  # This will not kill the process, just check if it's alive
#                     except ProcessLookupError:
#                         # Process is no longer running
#                         job_status[job_id] = "finished"
#                 else:
#                     # SLURM logic to check job status
#                     # You might use `squeue` or other SLURM commands to check job status
#                     # For simplicity, I'm just marking them as finished
#                     job_status[job_id] = "finished"  # Replace this with actual SLURM logic
#
#         running = sum(1 for status in job_status.values() if status == "running")
#         finished = sum(1 for status in job_status.values() if status == "finished")
#         failed = total_jobs - running - finished
#
#         # Display progress bar for completed jobs
#         logger.progress(
#             "Jobs Completion Progress",
#             finished,
#             total_jobs
#         )
#
#         # Display number of failed jobs
#         if failed > 0:
#             logger.log(f"Number of failed jobs: {failed}")
#
#         if running == 0:
#             break
#
#         time.sleep(4)  # Check every 30 seconds

def main_with_monitoring(make_final_cmd=None, allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX, HTTP_PREFIX)):
    # Define a flag to check if we should exit
        

    if make_final_cmd is None:
        make_final_cmd = make_final_cmd_slurm

    args = parse_args()

    use_socket = args.use_socket if args.use_socket is not None else False

    logger = make_logger()

    if use_socket:
        logger.log("User requested to use socket.")

    job_manager = JobManager(local_run = args.run_locally if args.run_locally is not None else False, open_socket = use_socket, logger=logger)
    
    # if OPEN_SOCKET:
    #     loop = asyncio.get_event_loop()
    #     loop.create_task(job_manager.start_server())
    #     loop.create_task(job_manager.process_messages())
    #     loop.run_forever()
    
    # Make sure server ip and port are set
    if job_manager.open_socket:
        logger.log("Trying to get server ip and port")
        while job_manager.server_ip is None or job_manager.server_port is None:
            time.sleep(1)

    csv_path_or_url = args.csv_path

    logger.log(f"Fetching csv from: {csv_path_or_url}")
    csv_path, spreadsheet_url, worksheet_name, gspread_client = fetch_csv(
        csv_path_or_url,
        logger
    )

    if args.expand:
        expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client)

    inputs_csv = read_csv_as_dict(csv_path)

    with mp.Manager() as shared_memory_manager:

        lock = shared_memory_manager.Lock()
        current_step = shared_memory_manager.Value("int", 0)
        shared_rows_to_run = shared_memory_manager.list()
        shared_csv_updates = shared_memory_manager.list()
        shared_default_config_paths = shared_memory_manager.dict()
        shared_row_numbers = shared_memory_manager.list()

        shared_jobs_dict = shared_memory_manager.dict()

        starmap_args_for_row_processing = [
            (
                make_final_cmd,
                csv_row,
                row_number,
                csv_path,
                args.conda_env,
                args.run_locally,
                args.log_file_path,
                spreadsheet_url,
                worksheet_name,
                logger,
                lock,
                shared_rows_to_run,
                shared_default_config_paths,
                shared_csv_updates,
                shared_row_numbers,
                current_step,
                len(inputs_csv),
                job_manager.server_ip,
                job_manager.server_port,
            )
            for row_number, csv_row in inputs_csv.items()
        ]

        if len(starmap_args_for_row_processing):

            # first_csv_row = starmap_args_for_row_processing[0][1]
            # check_csv_column_names(first_csv_row, allowed_prefixes)

            pool_size = get_pool_size(len(starmap_args_for_row_processing))

            upload_csv = False

            with mp.Pool(pool_size) as pool:

                pool.starmap(
                    process_csv_row,
                    starmap_args_for_row_processing
                )

                if len(shared_rows_to_run):

                    assert 2 * len(shared_rows_to_run) == len(shared_csv_updates)

                    concurrent_log_func = retrier_factory()(log_csv_for_concurrent)

                    concurrent_log_func(
                        csv_path,
                        shared_csv_updates
                    )

                    os.makedirs(os.path.dirname(args.log_file_path), exist_ok=True)

                    starmap_args_for_job_submitting = [
                        (
                            run_cmd,
                            args.log_file_path,
                            args.run_locally,
                            shared_jobs_dict,
                            row_id,
                            lock,
                            logger
                        )
                        for run_cmd, row_id in zip(shared_rows_to_run, shared_row_numbers)
                    ]

                    # pool.starmap(
                    #     submit_job,
                    #     starmap_args_for_job_submitting,
                    # )

                    async_results = []

                    for args_submit in starmap_args_for_job_submitting:
                        result = pool.apply_async(submit_job, args_submit)
                        # result.get()
                        async_results.append(result)

                    upload_csv = True

                    monitor_jobs_async(job_manager, async_results, shared_jobs_dict, args.run_locally, logger, spreadsheet_url,
                                       worksheet_name, shared_row_numbers, csv_path, gspread_client, lock,
                                       len(starmap_args_for_job_submitting))
            # Print all IDs
            # logger.log(f"Len of spawned jobs: {len(shared_job_objs)}")

            # for job_id in shared_job_objs:
            #     logger.log(f"Job ID: {job_id}")

            # if upload_csv:  # What's the function of this thing here?
            #     try_to_upload_csv(
            #         csv_path,
            #         spreadsheet_url,
            #         worksheet_name,
            #         gspread_client
            #     )
    # oh need to close the server and stuff bruv!!!!!!!!!!!!!!!!!!!!!
    if job_manager.open_socket:
        logger.log("Closing server")
        job_manager.server_process.terminate()
    logger.log("Job done, exiting.")
    sys.exit(0)


def get_pool_size(iterable_len):
    return min(
        min(
            max(1, mp.cpu_count() - 1),
            iterable_len
        ),
        MAX_PROCESSES
    )


# TODO: move these into the main function
run_jobs_flag = mp.Value('i', 1)  # 1 means jobs should run, 0 means they shouldn't
submitted_jobs = mp.Value('i', 0)
submitted_jobs_lock = mp.Lock()


def update_job_status(process, shared_jobs_dict, row_id, lock_manager, cancelled=False):
    """Update the job status and exit code in the shared dictionary."""
    with lock_manager:
        # print(f"Before update: {shared_jobs_dict[row_id]}")
        job_data = shared_jobs_dict[row_id].copy()  # Get a local copy
        if cancelled:
            job_data["status"] = JobStatus.CANCELLED
        else:
            if process.returncode == 0:
                job_data["status"] = JobStatus.COMPLETED
            else:
                job_data["status"] = JobStatus.FAILED
        job_data["exit_code"] = process.returncode
        shared_jobs_dict[row_id] = job_data  # Set the modified data back
        # print(f"After update: {shared_jobs_dict[row_id]}")


def submit_job(run_cmd, log_file_path, run_locally, shared_jobs_dict, row_id, lock_manager, logger):
    """

        row_id is used for identifying the job in the spreadsheet and for updating the status of the job in the shared memory.

    """

    if not run_jobs_flag.value:
        return "Job Stopped"

    def signal_handler(sig, frame):
        logger.log('KeyboardInterrupt caught, but continuing...')

    # Register the signal handler
    signal.signal(signal.SIGINT, signal_handler)

    with open(log_file_path, 'w+') as log_file:
        if run_locally:
            def get_main_and_child_pids(pid):
                main_pid = pid
                child_pids = [child.pid for child in psutil.Process(main_pid).children()]
                return main_pid, child_pids

            split_command = shlex.split(run_cmd)
            process = subprocess.Popen(split_command, stdout=log_file, stderr=log_file, shell=False)

            with submitted_jobs_lock:
                submitted_jobs.value += 1

            main_pid, child_pids = get_main_and_child_pids(process.pid)
            try:
                with lock_manager:
                    shared_jobs_dict[row_id] = {
                        "main_pid": main_pid,
                        "status": JobStatus.RUNNING
                    }
            except Exception as e:
                pass
            # Periodically check if the process is still running
            while process.poll() is None:
                # Check the run_jobs_flag during execution]
                if not run_jobs_flag.value:
                    logger.log(f"Stopping job with row ID: {row_id}")
                    # Send a SIGINT signal for graceful exit
                    process.send_signal(signal.SIGINT)

                    # Wait for up to 5 seconds for the process to exit gracefully
                    try:
                        process.wait(timeout=2)
                    except subprocess.TimeoutExpired:
                        # If the process doesn't exit within 5 seconds, terminate it forcefully
                        process.terminate()

                    # Update the job status after forcefully terminating
                    update_job_status(process, shared_jobs_dict, row_id, lock_manager, cancelled=True)
                    return 1
                    # return 404

                time.sleep(1)  # Sleep for a short duration before checking again

            # Process finished
            update_job_status(process, shared_jobs_dict, row_id, lock_manager)

            return process.returncode
        else:
            # SLURM logic
            # subprocess.call(
            #     run_cmd,
            #     stdout=log_file,
            #     stderr=log_file,
            #     shell=True
            # )

            timeout_duration = 60

            try:
                output = subprocess.check_output(run_cmd, stderr=subprocess.STDOUT, shell=True,
                                                 timeout=timeout_duration).decode('utf-8')
                numbers = re.findall(r"\d+", output)
                job_id = int(''.join(numbers))

                if job_id:
                    logger.log(f"Submitted a job with ID: {job_id}")
                else:
                    logger.log("Failed to extract job ID from the output.")

                logger.log(f"Job ID: {job_id}")
            except subprocess.CalledProcessError as e:
                print(f"Command failed with error: {e.output.decode('utf-8')}")
            except subprocess.TimeoutExpired:
                print(f"The command took longer than {timeout_duration} seconds to complete.")

            with submitted_jobs_lock:
                submitted_jobs.value += 1

            with lock_manager:
                shared_jobs_dict[row_id] = {
                    "job_id": job_id,
                    "status": "submitted",
                }

            return 0


def expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client):
    expanded_csv_path = os.path.join(
        os.path.dirname(csv_path),
        EXPANDED_CSV_PREFIX + os.path.basename(csv_path)
    )
    expand_csv(
        csv_path,
        expanded_csv_path
    )
    csv_path = expanded_csv_path
    if worksheet_name is not None:
        worksheet_name = EXPANDED_CSV_PREFIX + worksheet_name

    try_to_upload_csv(
        csv_path,
        spreadsheet_url,
        worksheet_name,
        gspread_client
    )


def fetch_default_config_path(path, logger):
    if FILES_URL in path:

        gdrive_client = make_gdrive_client(logger)
        with (
            NamedTemporaryFile('w+t', delete=False) as tmp_file
        ):
            remote_file = sync_local_file_with_gdrive(
                gdrive_client,
                tmp_file.name,
                path,
                download=True,
                logger=logger
            )

            file_path = os.path.join(
                get_default_configs_folder(),
                remote_file["title"].split('.')[0],
                f"default_config.yaml"
            )

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            shutil.move(tmp_file.name, file_path)

        return file_path

    else:
        return normalize_path(path)


def get_default_configs_folder():
    return os.path.join(
        get_project_root_path(),
        "experiment_configs"
    )


def process_csv_row(
        make_final_cmd,
        csv_row,
        row_number,
        input_csv_path,
        conda_env,
        run_locally,
        log_file_path,
        spreadsheet_url,
        worksheet_name,
        logger,
        lock,
        shared_rows_to_run,
        shared_default_config_paths,
        shared_csv_updates,
        shared_row_numbers,
        current_step,
        total_rows,
        server_ip,
        server_port,
):
    assert not spreadsheet_url or worksheet_name is not None, (
        "`worksheet_name` is None but this is not allowed when remote sheet is used;"
        "Make sure to pass the worksheet name using the `::` syntax in the --csv_path argument.")

    final_cmd = None

    whether_to_run = csv_row[WHETHER_TO_RUN_COLUMN]

    if (
            whether_to_run.isnumeric()
            and int(whether_to_run) != 0
    ):

        replace_placeholders(csv_row, CURRENT_ROW_PLACEHOLDER, str(row_number))
        replace_placeholders(csv_row, CURRENT_WORKSHEET_PLACEHOLDER, worksheet_name)

        default_config_path_or_url = csv_row[PATH_TO_DEFAULT_CONFIG_COLUMN]
        if not default_config_path_or_url in shared_default_config_paths:
            with lock:
                default_config_path = fetch_default_config_path(
                    default_config_path_or_url,
                    logger
                )
                shared_default_config_paths[default_config_path_or_url] \
                    = default_config_path
        else:
            default_config_path \
                = shared_default_config_paths[default_config_path_or_url]

        assert os.path.exists(default_config_path), f"Default config path at {default_config_path} does not exist."

        exp_dir = normalize_path(os.path.dirname(default_config_path))
        exp_name = os.path.basename(exp_dir)

        default_config = read_yaml(default_config_path)

        _, new_config_path = make_new_config(
            csv_row,
            row_number,
            input_csv_path,
            default_config,
            exp_dir,
            spreadsheet_url,
            worksheet_name,
            server_ip,
            server_port,
            run_locally
        )

        cmd_as_string = make_task_cmd(
            new_config_path,
            conda_env,
            normalize_path(csv_row[MAIN_PATH_COLUMN]),
            csv_row  # pass the row since might need to overwrite running command
        )

        log_folder = os.path.dirname(log_file_path)
        if not os.path.exists(log_folder):
            with lock:
                os.makedirs(log_folder, exist_ok=True)

        if run_locally:     
            # final_cmd = "{} &> {}".format(cmd_as_string, log_file_path)
            final_cmd = "{}".format(cmd_as_string)
        else:
            final_cmd = make_final_cmd(
                csv_row,
                exp_name,
                log_file_path,
                cmd_as_string
            )

    if final_cmd is not None:
        shared_csv_updates.append((row_number, STATUS_CSV_COLUMN, SUBMITTED_STATUS))
        shared_csv_updates.append((row_number, WHETHER_TO_RUN_COLUMN, "0"))
        shared_rows_to_run.append(final_cmd)
        shared_row_numbers.append(row_number)

    with lock:
        current_step.value += 1
        logger.progress(
            "Rows processing.",
            current_step.value,
            total_rows
        )


def make_final_cmd_slurm(csv_row, exp_name, log_file_path, cmd_as_string):
    slurm_args_dict = make_slurm_args_dict(
        csv_row,
        exp_name,
        log_file_path
    )
    if USE_SRUN:
        slurm_args_as_string = " ".join(
            [
                f"--{flag}={value}"
                for flag, value
                in slurm_args_dict.items()
            ]
        )
        final_cmd = "srun {} sh -c \"{}\" &".format(
            slurm_args_as_string,
            cmd_as_string
        )
    else:
        with (NamedTemporaryFile('w', delete=False)) as tmp_file:
            fill_sbatch_script(tmp_file, slurm_args_dict, cmd_as_string)
            final_cmd = "sbatch {}".format(tmp_file.name)

    return final_cmd


def check_csv_column_names(csv_row, allowed_prefixes):
    assert MAIN_PATH_COLUMN in csv_row

    assert WHETHER_TO_RUN_COLUMN in csv_row

    assert PATH_TO_DEFAULT_CONFIG_COLUMN in csv_row

    for i, key in enumerate(csv_row.keys()):
        assert key is not None, \
            f"Column {i} has empty column name. " \
            f"Or some table entries contain commas."
        if PREFIX_SEPARATOR in key:
            assert any([prefix in key for prefix in allowed_prefixes]), \
                f"\"{key}\" does not contain any of allowed prefixes " \
                f"from:\n{allowed_prefixes}\n"


def fill_sbatch_script(sbatch_file, slurm_args_dict, command):
    sbatch_file.write("#!/bin/bash\n")

    for slurm_arg, value in slurm_args_dict.items():
        sbatch_file.write("#SBATCH --{}={}\n".format(slurm_arg, value))

    sbatch_file.write(command)
    sbatch_file.flush()


def make_new_config(
        csv_row,
        row_number,
        input_csv_path,
        default_config,
        exp_dir,
        spreadsheet_url,
        worksheet_name,
        server_ip,
        server_port,
        run_locally
):
    """
    Assume that `fixed_params` are the ones being passed to the jobs. We will move them to the config file
    but will update them according to the `deltas` in the config file.
    """
    deltas = extract_from_csv_row_by_prefix(
        csv_row,
        DELTA_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        """ Make changes only in the 'fixed_params' subdict of the config """
        assert "fixed_params" in default_config, \
            "If DELTA_AFFECTS_ONLY_FIXED_PARAMS is True, then 'fixed_params' must be in default_config."
        for key in list(deltas.keys()):
            deltas["fixed_params" + NESTED_CONFIG_KEY_SEPARATOR + key] = deltas[key]
            del deltas[key]

    if len(deltas) > 0:
        check_duplicates(list(deltas.keys()))


    deltas_del = []
    for key in deltas.keys():
        value = deltas[key]
        if value == EMPTY_STRING:
            deltas[key] = ""
        elif value == "":
            if EMPTY_VALUE_MEANS_NO_CHANGE:
                # erase the key
                deltas_del.append(key)
                warnings.warn("WARNING: Empty value for {} will be ignored.".format(key))
            else:
                raise Exception(f"Empty value for {make_delta_column_name(key)}")
    for key in deltas_del:
        del deltas[key]

    decode_strings_in_dict(
        deltas,
        list_separators=[' '],
        list_start_symbol='[',
        list_end_symbol=']'
    )

    deltas["logging/output_csv"] = make_csv_config(
        input_csv_path,
        row_number,
        spreadsheet_url,
        worksheet_name
    )
    
    deltas["logging/server_ip"] = server_ip
    deltas["logging/server_port"] = server_port

    deltas["run_locally"] = run_locally
        
    new_config = make_config_from_default_and_deltas(default_config, deltas)
    # make sure we preserve deltas though
    for delta in deltas:
        if delta == "logging/output_csv":
            continue
        new_config[DELTA_PREFIX + PREFIX_SEPARATOR + delta] = deltas[delta]

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        # Copy stuff from `fixed_params` to the root of the config
        for key, value in new_config["fixed_params"].items():
            new_config[key] = value

    new_config_path = os.path.join(
        exp_dir,
        AUTOGEN_PREFIX,
        make_autogenerated_config_name(input_csv_path, row_number)
    )
    os.makedirs(os.path.dirname(new_config_path), exist_ok=True)
    save_as_yaml(
        new_config_path,
        new_config
    )
    return new_config, new_config_path


def replace_placeholders(csv_row, placeholder, new_value):
    for column_name, value in csv_row.items():
        csv_row[column_name] = str(value).replace(placeholder, new_value)


def make_task_cmd(new_config_path, conda_env, exec_path, csv_row):
    exec_args = "--config_path {}".format(new_config_path)

    # If `custom_run_cmd` is passed, overwrite the default command
    # with the custom one.
    if "custom_run_cmd" in csv_row:
        cmd = "{} {} {}".format(csv_row["custom_run_cmd"], exec_path, exec_args)
    else:
        cmd = "{} {} && python {} {}".format(
            NEW_SHELL_INIT_COMMAND,
            conda_env,
            exec_path,
            exec_args
        )
    return cmd


def make_slurm_args_dict(csv_row, exp_name, log_file):
    all_slurm_args_dict = copy.deepcopy(DEFAULT_SLURM_ARGS_DICT)

    all_slurm_args_dict["job-name"] = exp_name

    all_slurm_args_dict["output"] = log_file
    all_slurm_args_dict["error"] = log_file

    specified_slurm_args = extract_from_csv_row_by_prefix(
        csv_row,
        SLURM_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    all_slurm_args_dict |= specified_slurm_args

    os.makedirs(os.path.dirname(all_slurm_args_dict["output"]), exist_ok=True)
    os.makedirs(os.path.dirname(all_slurm_args_dict["error"]), exist_ok=True)

    return all_slurm_args_dict


def extract_from_csv_row_by_prefix(csv_row, prefix, ignore_values):
    prefix_len = len(prefix)
    result = {}
    for key, value in csv_row.items():
        assert key is not None, "Possibly inconsistent number of delimeters."
        if key == prefix:
            raise Exception(
                f"Found \"{prefix}\" (nothing after this prefix) "
                f"in csv_row:\n{csv_row}"
            )
        if (
                len(key) > prefix_len
                and
                prefix == key[:prefix_len]
                and
                not value in ignore_values
        ):
            result[key[prefix_len:]] = value

    return result


def make_config_from_default_and_deltas(default_config, deltas):
    assert isinstance(deltas, dict)
    new_config = copy.deepcopy(default_config)
    for nested_config_key, new_value in deltas.items():
        update_dict_by_nested_key(
            new_config,
            nested_config_key.split(NESTED_CONFIG_KEY_SEPARATOR),
            new_value,
            to_create_new_elements=True
        )
    return new_config


if __name__ == "__main__":
    main_with_monitoring()
