import argparse
import os
import copy
from tempfile import NamedTemporaryFile
import subprocess
import shutil
import sys
import multiprocessing as mp


# local modules
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from utility.utils import (
    DEFAULT_ENV_NAME,
    NEW_SHELL_INIT_COMMAND,
    make_autogenerated_config_name,
    read_yaml,
    save_as_yaml,
    update_dict_by_nested_key,
    get_project_root_path,
    decode_strings_in_dict,
    read_csv_as_dict,
    normalize_path,
    check_duplicates,
    expand_csv,
    retrier_factory,
    is_number,
    range_for_each_group,
    get_with_assert
)
from utility.configs import (
    AUTOGEN_PREFIX,
    NESTED_CONFIG_KEY_SEPARATOR,
    make_csv_config
)
from utility.logger import (
    STATUS_CSV_COLUMN,
    SUBMITTED_STATUS,
    WHETHER_TO_RUN_COLUMN,
    DELTA_PREFIX,
    SLURM_PREFIX,
    CONDOR_PREFIX,
    CONDOR_BID_KEY,
    ENV_VAR_PREFIX,
    PREFIX_SEPARATOR,
    PLACEHOLDERS_FOR_DEFAULT,
    make_logger,
    make_gdrive_client,
    sync_local_file_with_gdrive,
    log_csv_for_concurrent,
    fetch_csv,
    try_to_upload_csv,
    make_delta_column_name
)


FILES_URL = "https://drive.google.com/file"


USE_SRUN = False


PATH_TO_DEFAULT_CONFIG_COLUMN = "path_to_default_config"
MAIN_PATH_COLUMN = "path_to_main"
DEV_NULL = "/dev/null"
DEFAULT_SLURM_ARGS_DICT = {
    "partition": "<DEFAULT PARTITION>",
    "gres": "gpu:1",
    "time": "02:00:00",
    "ntasks": 1,
    "cpus-per-task": 2,
    "error": DEV_NULL,
    "output": DEV_NULL
}
DEFAULT_CONDOR_ARGS_DICT = {

}
EMPTY_STRING = "EMPTY_STRING"
EXPANDED_CSV_PREFIX = "expanded_"
CURRENT_ROW_PLACEHOLDER = "__ROW__"
CURRENT_WORKSHEET_PLACEHOLDER = "__WORKSHEET__"
RUNNER_PLACEHOLDER = "__RUNNER__"
COMMA_PLACEHOLDER = "__COMMA__"
PATH_TO_RUNNER_MAIN = os.path.join(
    os.path.dirname(os.path.dirname(__file__)),
    "run_cmd",
    "main.py"
)
MAX_PROCESSES = 16


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train and/or validate models."
    )
    parser.add_argument(
        "--csv_path",
        type=str,
        required=True,
        help="path to csv file"
    )
    # parser.add_argument(
    #     "--python_path",
    #     type=str,
    #     required=False,
    #     help="path to python executable"
    # )
    parser.add_argument(
        "--cluster_type",
        type=str,
        required=False,
        choices=["slurm", "condor"],
        help="cluster type",
        default="slurm"
    )
    parser.add_argument(
        "--conda_env",
        type=str,
        required=False,
        default=None,
        help="conda environment name"
    )
    parser.add_argument(
        "--run_locally",
        action="store_true",
        help="whether to run this script locally"
    )
    parser.add_argument(
        "--log_file_path",
        type=str,
        required=False,
        default=get_default_log_file_path(),
        help="default path for the log file"
    )
    parser.add_argument(
        "--expand",
        action="store_true",
        help="whether to first expand input csv by cartesian product of options"
    )
    parser.add_argument(
        "--n_groups",
        type=str,
        help=(
            "in how many groups to group all jobs; "
            "if 0 - then use one group per job"
        ),
        default="0"
    )
    return parser.parse_args()


def get_default_log_file_path():
    return os.path.join(
        get_project_root_path(),
        "tmp",
        "tmp_log_for_run_from_csv.out"
    )


def main():

    args = parse_args()

    # make_final_cmd=None, allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX)
    if args.cluster_type == "slurm":
        assert args.conda_env is not None, \
            "Conda env is required for Slurm"
        make_final_cmd = make_final_cmd_slurm
        allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX)
    elif args.cluster_type == "condor":
        assert args.conda_env is not None, \
            "Conda env is required for Condor"
        make_final_cmd = make_final_cmd_condor
        allowed_prefixes=(CONDOR_PREFIX, DELTA_PREFIX, ENV_VAR_PREFIX)
    else:
        raise Exception(f"Unknown cluster type: {args.cluster_type}")

    # if make_final_cmd is None:
    #     make_final_cmd = make_final_cmd_slurm

    # TODO(Alex | 26.01.2024): remove once tested for remote runs
    if not args.run_locally:
        assert args.n_groups == "0", \
            "n_groups > 0 is not supported for non-local runs"

    logger = make_logger()

    csv_path_or_url = args.csv_path

    logger.log(f"Fetching csv from: {csv_path_or_url}")
    csv_path, spreadsheet_url, worksheet_name, gspread_client = fetch_csv(
        csv_path_or_url,
        logger
    )

    if args.expand:
        expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client)

    inputs_csv = read_csv_as_dict(csv_path)

    with mp.Manager() as shared_memory_manager:

        lock = shared_memory_manager.Lock()
        current_step = shared_memory_manager.Value("int", 0)
        shared_rows_to_run = shared_memory_manager.list()
        shared_csv_updates = shared_memory_manager.list()
        shared_default_config_paths = shared_memory_manager.dict()

        starmap_args_for_row_processing = [
            (
                make_final_cmd,
                csv_row,
                row_number,
                csv_path,
                args.conda_env,
                args.run_locally,
                args.log_file_path,
                spreadsheet_url,
                worksheet_name,
                logger,
                lock,
                shared_rows_to_run,
                shared_default_config_paths,
                shared_csv_updates,
                current_step,
                len(inputs_csv),
                # args.python_path
            )
                for row_number, csv_row in inputs_csv.items()
        ]

        if len(starmap_args_for_row_processing):

            first_csv_row = starmap_args_for_row_processing[0][1]
            check_csv_column_names(first_csv_row, allowed_prefixes)

            pool_size = get_pool_size(len(starmap_args_for_row_processing))

            upload_csv = False

            with mp.Pool(pool_size) as pool:

                pool.starmap(
                    process_csv_row,
                    starmap_args_for_row_processing
                )

                if len(shared_rows_to_run):

                    assert 2 * len(shared_rows_to_run) == len(shared_csv_updates)

                    concurrent_log_func = retrier_factory()(log_csv_for_concurrent)

                    concurrent_log_func(
                        csv_path,
                        shared_csv_updates
                    )

                    log_file_path = args.log_file_path
                    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
                    logger.log(
                        f"Logs from this submission script "
                        f"will be stored in: {log_file_path}"
                    )

                    n_groups = int(args.n_groups)
                    if n_groups != 0:

                        shared_rows_to_run = merge_jobs_in_groups(
                            shared_rows_to_run,
                            n_groups
                        )

                    starmap_args_for_job_submitting = [
                        (
                            run_cmd,
                            log_file_path
                        )
                            for run_cmd in shared_rows_to_run
                    ]

                    pool.starmap(
                        submit_job,
                        starmap_args_for_job_submitting
                    )
                    upload_csv = True

            if upload_csv:
                try_to_upload_csv(
                    csv_path,
                    spreadsheet_url,
                    worksheet_name,
                    gspread_client
                )


def merge_jobs_in_groups(cmds_to_run, n_groups):

    def merge_cmds(cmds):
        result = ""
        for i, cmd in enumerate(cmds):
            assert len(cmd) > 2
            assert cmd[-2:] == " &"
            result += cmd[:-2]
            if i + 1 < len(cmds):
                result += " ; "
        return f"nohup bash -c \'{result}\' &> {DEV_NULL} &"

    merged_cmds = []
    ranges = range_for_each_group(n_groups, len(cmds_to_run))
    for range in ranges:
        merged_cmds.append(merge_cmds(cmds_to_run[range[0]:range[1]]))
    return merged_cmds


def get_pool_size(iterable_len):
    return min(
        min(
            max(1, mp.cpu_count() - 1),
            iterable_len
        ),
        MAX_PROCESSES
    )


def submit_job(run_cmd, log_file_path):
    print(f"Submitting job: {run_cmd}") # debug
    # raise Exception("Stop here, this is not a real error, we just want to see the job command") # debug
    with open(log_file_path, 'w+') as log_file:
        subprocess.call(
            run_cmd,
            stdout=log_file,
            stderr=log_file,
            shell=True
        )


def expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client):

    expanded_csv_path = os.path.join(
        os.path.dirname(csv_path),
        EXPANDED_CSV_PREFIX + os.path.basename(csv_path)
    )
    expand_csv(
        csv_path,
        expanded_csv_path
    )
    csv_path = expanded_csv_path
    if worksheet_name is not None:
        worksheet_name = EXPANDED_CSV_PREFIX + worksheet_name

    try_to_upload_csv(
        csv_path,
        spreadsheet_url,
        worksheet_name,
        gspread_client
    )


def fetch_default_config_path(path, logger):

    if FILES_URL in path:

        gdrive_client = make_gdrive_client(logger)
        with (
            NamedTemporaryFile('w+t', delete=False) as tmp_file
        ):
            remote_file = sync_local_file_with_gdrive(
                gdrive_client,
                tmp_file.name,
                path,
                download=True,
                logger=logger
            )

            file_path = os.path.join(
                get_default_configs_folder(),
                remote_file["title"].split('.')[0],
                f"default_config.yaml"
            )

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            shutil.move(tmp_file.name, file_path)

        return file_path

    else:
        return normalize_path(path)


def get_default_configs_folder():
    return os.path.join(
        get_project_root_path(),
        "experiment_configs"
    )


def process_csv_row(
    make_final_cmd,
    csv_row,
    row_number,
    input_csv_path,
    conda_env,
    run_locally,
    log_file_path,
    spreadsheet_url,
    worksheet_name,
    logger,
    lock,
    shared_rows_to_run,
    shared_default_config_paths,
    shared_csv_updates,
    current_step,
    total_rows,
    # python_path
):

    final_cmd = None

    whether_to_run = csv_row[WHETHER_TO_RUN_COLUMN]

    if (
        is_number(whether_to_run)
            and int(whether_to_run) != 0
    ):

        replace_placeholders(csv_row, CURRENT_ROW_PLACEHOLDER, str(row_number))
        replace_placeholders(csv_row, CURRENT_WORKSHEET_PLACEHOLDER, worksheet_name)
        replace_placeholders(csv_row, RUNNER_PLACEHOLDER, PATH_TO_RUNNER_MAIN)
        replace_placeholders(csv_row, COMMA_PLACEHOLDER, ",")

        default_config_path_or_url = csv_row[PATH_TO_DEFAULT_CONFIG_COLUMN]
        if not default_config_path_or_url in shared_default_config_paths:
            with lock:
                default_config_path = fetch_default_config_path(
                    default_config_path_or_url,
                    logger
                )
                shared_default_config_paths[default_config_path_or_url] \
                    = default_config_path
        else:
            default_config_path \
                = shared_default_config_paths[default_config_path_or_url]

        assert os.path.exists(default_config_path), \
            f"Default config path does not exist: {default_config_path}"

        exp_dir = normalize_path(os.path.dirname(default_config_path))
        exp_name = os.path.basename(exp_dir)

        default_config = read_yaml(default_config_path)

        _, new_config_path = make_new_config(
            csv_row,
            row_number,
            input_csv_path,
            default_config,
            exp_dir,
            spreadsheet_url,
            worksheet_name
        )
        exp_exec_path = normalize_path(csv_row[MAIN_PATH_COLUMN])
        cmd_as_string = make_task_cmd(
            new_config_path,
            conda_env,
            exp_exec_path
        )

        log_folder = os.path.dirname(log_file_path)
        if not os.path.exists(log_folder):
            with lock:
                os.makedirs(log_folder, exist_ok=True)

        if run_locally:
            final_cmd = "{} &> {} &".format(cmd_as_string, log_file_path)
        else:
            final_cmd = make_final_cmd(
                csv_row,
                exp_name,
                log_file_path,
                cmd_as_string,
                exp_config_path=new_config_path,
                exp_main_path=exp_exec_path,
                conda_env=conda_env,
                # python_path=python_path
            )

    if final_cmd is not None:

        shared_csv_updates.append((row_number, STATUS_CSV_COLUMN, SUBMITTED_STATUS))
        shared_csv_updates.append((row_number, WHETHER_TO_RUN_COLUMN, "0"))
        shared_rows_to_run.append(final_cmd)

    with lock:
        current_step.value += 1
        logger.progress(
            "Rows processing.",
            current_step.value,
            total_rows
        )


# def fill_condor_sh_script(condor_sh_file):

#     condor_sh_file.write("#!/bin/bash\n")

#     # for slurm_arg, value in slurm_args_dict.items():
#     #     sbatch_file.write("#SBATCH --{}={}\n".format(slurm_arg, value))

#     # sbatch_file.write(command)
#     condor_sh_file.flush()



def fill_condor_sh_script(
    condor_sh_file,
    conda_env,
    env_vars,
    exp_main_path,
    exp_config_path
):
    """Fill shell script with required commands and environment setup."""
    condor_sh_file.write("#!/bin/bash\n")
    condor_sh_file.write("nvidia-smi\n\n")

    # Load environment
    condor_sh_file.write("# Load environment\n")
    condor_sh_file.write(f"source {os.path.expanduser('~/python/python/etc/profile.d/conda.sh')}\n")
    condor_sh_file.write(f"conda activate {conda_env}\n\n")

    # Environment variables

    # Define working directory
    for env_var, value in env_vars.items():
        condor_sh_file.write(f"export {env_var}={value}\n")
    # condor_sh_file.write(f"export WORK_DIR={os.path.expanduser('~/jailbreak_filter/llm-threat-model-private')}\n")
    # condor_sh_file.write(f"export CUDA_HOME={os.path.expanduser('~/software/nvidia/cuda-12.1')}\n")
    # condor_sh_file.write("export LD_LIBRARY_PATH=/is/software/nvidia/cuda-12.1/lib64\n")
    # condor_sh_file.write("export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n")
    # condor_sh_file.write("export WANDB_SERVICE_WAIT=300\n")
    # condor_sh_file.write(f"export TRANSFORMERS_CACHE={os.path.expanduser('~/huggingface/hub/models')}\n")
    # condor_sh_file.write(f"export HF_HOME={os.path.expanduser('~/huggingface')}\n")
    # condor_sh_file.write(f"export VLLM_NCCL_SO_PATH={os.path.expanduser('~/python/python/envs/jailbreak_filter/lib/python3.10/site-packages/nvidia/nccl/lib/libnccl.so.2')}\n")
    # condor_sh_file.write('export HOST_IP=$(hostname -I | awk \'{print $1}\')\n')
    # condor_sh_file.write("export TRANSFORMERS_OFFLINE=1\n")
    # condor_sh_file.write("export RAY_ADDRESS=${HOST_IP}:6354\n\n")

    # Change directory
    condor_sh_file.write('cd $WORK_DIR || exit 1\n\n')

    # # Set variables
    # condor_sh_file.write('base_dir="results_new"\n')
    # condor_sh_file.write('experiment_name="${experiment}"\n')
    # condor_sh_file.write('behaviors_path="./data/behavior_datasets/harmbench_behaviors_all_no_copyright.csv"\n')
    # condor_sh_file.write('#behaviors_path="./data/adv_bench.csv"\n')
    # condor_sh_file.write('save_dir=$WORK_DIR/${base_dir}/${method_name}/${experiment}/test_cases\n\n')

    # # Echo parameters
    # condor_sh_file.write('echo "method_name=$method_name"\n')
    # condor_sh_file.write('echo "experiment_name=$experiment_name"\n')
    # condor_sh_file.write('echo "behaviors_path=$behaviors_path"\n')
    # condor_sh_file.write('echo "save_dir=$save_dir"\n')
    # condor_sh_file.write('echo "subset=[$start_idx, $end_idx]"\n')
    # condor_sh_file.write('echo "run_id=$run_id"\n')
    # condor_sh_file.write('echo "seed=$seed"\n')
    # condor_sh_file.write('#echo "search_width=$search_width"\n')
    # condor_sh_file.write('echo "jb_filter_window_size=$jb_ws"\n\n')

    # # Conditional Ray setup
    # condor_sh_file.write('if [ "$method_name" = "PAIR" ]; then\n')
    # condor_sh_file.write(f'    export PATH="{os.path.expanduser("~/.local/bin")}:$PATH"\n')
    # condor_sh_file.write('    ray start --head --port=6354\n')
    # condor_sh_file.write('    ray status\n')
    # condor_sh_file.write('fi\n\n')

    # Construct the command
    # condor_sh_file.write("# Construct the command\n")
    condor_sh_file.write(f'cmd="python -u {exp_main_path} \\\n')
    condor_sh_file.write(f'    --config_path {exp_config_path} "\n')
    condor_sh_file.write(f'eval $cmd\n')

    condor_sh_file.flush()


# def fill_condor_sub_script(condor_sub_file, condor_sh_file_path):

#     condor_sub_file.write("???\n")

#     # for slurm_arg, value in slurm_args_dict.items():
#     #     sbatch_file.write("#SBATCH --{}={}\n".format(slurm_arg, value))

#     # sbatch_file.write(command)
#     condor_sub_file.flush()


def fill_condor_sub_script(
    condor_sub_file,
    condor_sh_file_path,
    condor_args_dict
):
    """Fill HTCondor submission script with required parameters."""
    condor_sub_file.write("##################################\n")
    condor_sub_file.write("# HTCondor submission file for Python script execution with the arguments\n")
    condor_sub_file.write("##################################\n\n")

    condor_sub_file.write(f"executable = {condor_sh_file_path}\n")
    os.system(f'chmod +x {condor_sh_file_path}')
    # condor_sub_file.write(f"arguments = {condor_sh_file_path} $(input_argument1) $(input_argument2)\n\n")

    # # Standard log files
    # condor_sub_file.write("error = log/job.$(Cluster).$(Process).err\n")
    # condor_sub_file.write("output = log/job.$(Cluster).$(Process).out\n")
    # condor_sub_file.write("log = log/job.$(Cluster).$(Process).log\n\n")

    # # Resource requests
    # condor_sub_file.write("request_memory = 116GB\n")
    # condor_sub_file.write("request_cpus = 1\n")
    # condor_sub_file.write("request_gpus = 1\n\n")

    # # Requirements
    # condor_sub_file.write("requirements = (CUDAGlobalMemoryMb >= 40000)\n\n")

    # # GPU requirements
    # condor_sub_file.write('# && (CUDACapability >= 8.0)\n')
    # condor_sub_file.write('requirements = (CUDADeviceName == "NVIDIA A100-SXM4-80GB" || # && CUDADeviceName == "NVIDIA A100-SXM4-80GB" || # && CUDADeviceName == "NVIDIA H100" || # && CUDADeviceName == "NVIDIA H800 HBM3")\n\n')

    # # Job management
    # condor_sub_file.write("MaxTime = 19999\n")
    # condor_sub_file.write("periodic_remove = (JobStatus =?= 2) && ((CurrentTime - JobCurrentStartDate) >= $(MaxTime))\n")
    # condor_sub_file.write('JobBatchName = "jupyter"\n\n')

    for condor_arg, value in condor_args_dict.items():
        condor_sub_file.write(f"{condor_arg} = {value}\n")

    # Queue section
    condor_sub_file.write("##################################\n")
    # condor_sub_file.write("# Передача двух аргументов через конструкцию Queue\n")
    # condor_sub_file.write("# Если значений не совпадает размеры файлов нельзя не использовать.\n")
    # condor_sub_file.write("Queue input_argument1, input_argument2 from (\n")
    # condor_sub_file.write("    значение_аргумента1 значение_аргумента2\n")
    # condor_sub_file.write(")\n")
    condor_sub_file.write("queue\n")

    condor_sub_file.flush()


def make_final_cmd_condor(
    csv_row,
    exp_name,
    log_file_path,
    cmd_as_string,
    exp_config_path,
    exp_main_path,
    conda_env,
    # python_path
):
    condor_args_dict = make_condor_args_dict(
        csv_row,
        exp_name,
        log_file_path
    )
    env_vars = extract_from_csv_row_by_prefix(
        csv_row,
        ENV_VAR_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )
    bid = get_with_assert(condor_args_dict, CONDOR_BID_KEY)
    condor_args_dict.pop(CONDOR_BID_KEY)

    tmp_dir_for_sh_file = os.path.join(
        os.path.expanduser('~'),
        "tmp_sh_files_for_condor"
    )
    os.makedirs(tmp_dir_for_sh_file, exist_ok=True)
    with (NamedTemporaryFile('w', delete=False, dir=tmp_dir_for_sh_file)) as tmp_sh_file:
        fill_condor_sh_script(
            condor_sh_file=tmp_sh_file,
            conda_env=conda_env,
            env_vars=env_vars,
            exp_main_path=exp_main_path,
            exp_config_path=exp_config_path
        )
        condor_sh_path = tmp_sh_file.name

    with (NamedTemporaryFile('w', delete=False)) as tmp_sub_file:
        fill_condor_sub_script(
            condor_sub_file=tmp_sub_file,
            condor_sh_file_path=condor_sh_path,
            condor_args_dict=condor_args_dict
        )
        condor_sub_path = tmp_sub_file.name

    final_cmd = f"condor_submit_bid {bid} {condor_sub_path}"
    return final_cmd


def make_final_cmd_slurm(
    csv_row,
    exp_name,
    log_file_path,
    cmd_as_string,
    exp_config_path=None,
    exp_main_path=None,
    conda_env=None,
    # python_path=None
):

    slurm_args_dict = make_slurm_args_dict(
        csv_row,
        exp_name,
        log_file_path
    )
    if USE_SRUN:
        slurm_args_as_string = " ".join(
            [
                f"--{flag}={value}"
                    for flag, value
                        in slurm_args_dict.items()
            ]
        )
        final_cmd = "srun {} sh -c \"{}\" &".format(
            slurm_args_as_string,
            cmd_as_string
        )
    else:
        with (NamedTemporaryFile('w', delete=False)) as tmp_file:
            fill_sbatch_script(tmp_file, slurm_args_dict, cmd_as_string)
            final_cmd = "sbatch {}".format(tmp_file.name)

    return final_cmd


def check_csv_column_names(csv_row, allowed_prefixes):

    assert MAIN_PATH_COLUMN in csv_row

    assert WHETHER_TO_RUN_COLUMN in csv_row

    assert PATH_TO_DEFAULT_CONFIG_COLUMN in csv_row

    for i, key in enumerate(csv_row.keys()):
        assert key is not None, \
            f"Column {i} has empty column name. " \
            f"Or some table entries contain commas."
        if PREFIX_SEPARATOR in key:
            assert any([prefix in key for prefix in allowed_prefixes]), \
                f"\"{key}\" does not contain any of allowed prefixes " \
                f"from:\n{allowed_prefixes}\n"


def fill_sbatch_script(sbatch_file, slurm_args_dict, command):

    sbatch_file.write("#!/bin/bash\n")

    for slurm_arg, value in slurm_args_dict.items():
        sbatch_file.write("#SBATCH --{}={}\n".format(slurm_arg, value))

    sbatch_file.write(command)
    sbatch_file.flush()


def make_new_config(
    csv_row,
    row_number,
    input_csv_path,
    default_config,
    exp_dir,
    spreadsheet_url,
    worksheet_name
):

    deltas = extract_from_csv_row_by_prefix(
        csv_row,
        DELTA_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    for key in deltas.keys():
        value = deltas[key]
        if value == EMPTY_STRING:
            deltas[key] = ""
        elif value == "":
            raise Exception(f"Empty value for {make_delta_column_name(key)}")

    decode_strings_in_dict(
        deltas,
        list_separators=[' '],
        list_start_symbol='[',
        list_end_symbol=']'
    )

    deltas["logging/output_csv"] = make_csv_config(
        input_csv_path,
        row_number,
        spreadsheet_url,
        worksheet_name
    )
    new_config = make_config_from_default_and_deltas(default_config, deltas)
    new_config_path = os.path.join(
        exp_dir,
        AUTOGEN_PREFIX,
        make_autogenerated_config_name(input_csv_path, row_number)
    )
    os.makedirs(os.path.dirname(new_config_path), exist_ok=True)
    save_as_yaml(
        new_config_path,
        new_config
    )
    return new_config, new_config_path


def replace_placeholders(csv_row, placeholder, new_value):
    for column_name, value in csv_row.items():
        if placeholder in str(value):
            csv_row[column_name] = str(value).replace(placeholder, new_value)


def make_task_cmd(new_config_path, conda_env, exec_path):
    exec_args = "--config_path {}".format(new_config_path)
    return "{} {} && python {} {}".format(
        NEW_SHELL_INIT_COMMAND,
        conda_env,
        exec_path,
        exec_args
    )


def make_condor_args_dict(csv_row, exp_name, log_file):

    all_condor_args_dict = copy.deepcopy(DEFAULT_CONDOR_ARGS_DICT)

    # all_slurm_args_dict["job-name"] = exp_name

    # all_slurm_args_dict["output"] = log_file
    # all_slurm_args_dict["error"] = log_file

    specified_condor_args = extract_from_csv_row_by_prefix(
        csv_row,
        CONDOR_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    all_condor_args_dict |= specified_condor_args

    # os.makedirs(os.path.dirname(all_slurm_args_dict["output"]), exist_ok=True)
    # os.makedirs(os.path.dirname(all_slurm_args_dict["error"]), exist_ok=True)

    make_folder_if_needed(get_with_assert(all_condor_args_dict, "output"))
    make_folder_if_needed(get_with_assert(all_condor_args_dict, "error"))

    return all_condor_args_dict


def make_folder_if_needed(path):
    folder_path = os.path.dirname(path)
    if folder_path != '':
        os.makedirs(folder_path, exist_ok=True)


def make_slurm_args_dict(csv_row, exp_name, log_file):


    all_slurm_args_dict = copy.deepcopy(DEFAULT_SLURM_ARGS_DICT)

    all_slurm_args_dict["job-name"] = exp_name

    all_slurm_args_dict["output"] = log_file
    all_slurm_args_dict["error"] = log_file

    specified_slurm_args = extract_from_csv_row_by_prefix(
        csv_row,
        SLURM_PREFIX + PREFIX_SEPARATOR,
        ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    all_slurm_args_dict |= specified_slurm_args

    make_folder_if_needed(all_slurm_args_dict["output"])
    make_folder_if_needed(all_slurm_args_dict["error"])

    return all_slurm_args_dict


def extract_from_csv_row_by_prefix(csv_row, prefix, ignore_values):

    prefix_len = len(prefix)
    result = {}
    for key, value in csv_row.items():
        assert key is not None, "Possibly inconsistent number of delimeters."
        if key == prefix:
            raise Exception(
                f"Found \"{prefix}\" (nothing after this prefix) "
                f"in csv_row:\n{csv_row}"
            )
        if (
                len(key) > prefix_len
            and
                prefix == key[:prefix_len]
            and
                not value in ignore_values
        ):
            result[key[prefix_len:]] = value

    return result


def make_config_from_default_and_deltas(default_config, deltas):
    assert isinstance(deltas, dict)
    new_config = copy.deepcopy(default_config)
    for nested_config_key, new_value in deltas.items():
        update_dict_by_nested_key(
            new_config,
            nested_config_key.split(NESTED_CONFIG_KEY_SEPARATOR),
            new_value,
            to_create_new_elements=True
        )
    return new_config


if __name__ == "__main__":
    main()
