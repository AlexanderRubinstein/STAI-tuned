import argparse
import datetime
import os
import copy
import re
import shlex
import signal
import time
import warnings
import traceback
from tempfile import NamedTemporaryFile
import subprocess
import shutil
import sys
import multiprocessing as mp
from typing import Dict, List
from warnings import warn

import psutil
from stuned.gsheet_batch_updater import GSheetBatchUpdater
from stuned.job import Job, JobStatus, find_job_id_by_row_id, find_job_idx, get_slurm_job_status
from stuned.job_manager import JobManager
from stuned.utility.ai_center_cluster_specific import get_region, CLUSTER
from stuned.utility.local_processing_utils import process_exists

# local modules
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from stuned.utility.utils import (
    DEFAULT_ENV_NAME,
    NEW_SHELL_INIT_COMMAND,
    make_autogenerated_config_name,
    read_yaml,
    save_as_yaml,
    update_dict_by_nested_key,
    get_project_root_path,
    decode_strings_in_dict,
    read_csv_as_dict,
    normalize_path,
    check_duplicates,
    expand_csv,
    retrier_factory,
    is_number,
)
from stuned.utility.configs import make_csv_config
from stuned.utility.constants import (
    AUTOGEN_PREFIX,
    NESTED_CONFIG_KEY_SEPARATOR,
    STATUS_CSV_COLUMN,
    SUBMITTED_STATUS,
    WHETHER_TO_RUN_COLUMN,
)
from stuned.utility.logger import (
    DELTA_PREFIX,
    SLURM_PREFIX,
    PREFIX_SEPARATOR,
    PLACEHOLDERS_FOR_DEFAULT,
    make_logger,
    make_gdrive_client,
    sync_local_file_with_gdrive,
    log_csv_for_concurrent,
    fetch_csv,
    try_to_upload_csv,
    make_delta_column_name,
    HTTP_PREFIX,
    GspreadClient,
)

# Arnas' changes
DELTA_AFFECTS_ONLY_FIXED_PARAMS = True
EMPTY_VALUE_MEANS_NO_CHANGE = True

##

FILES_URL = "https://drive.google.com/file"

USE_SRUN = False

OPEN_SOCKET = True

PATH_TO_DEFAULT_CONFIG_COLUMN = "path_to_default_config"
MAIN_PATH_COLUMN = "path_to_main"
DEV_NULL = "/dev/null"
DEFAULT_SLURM_ARGS_DICT = {
    # "partition": "gpu-2080ti-beegfs",
    "gpus": 1,
    "time": "24:00:00",
    "ntasks": 1,
    "cpus-per-task": 2,
    "error": DEV_NULL,
    "output": DEV_NULL,
}
EMPTY_STRING = "EMPTY_STRING"
EXPANDED_CSV_PREFIX = "expanded_"
CURRENT_ROW_PLACEHOLDER = "__ROW__"
CURRENT_WORKSHEET_PLACEHOLDER = "__WORKSHEET__"
MAX_PROCESSES = 16

# Monitor-related constants
MONITOR_STATUS_COLUMN = "status_monitor"
MONITOR_EXIT_CODE_COLUMN = "exit_code_monitor"
MONITOR_JOB_ID_COLUMN = "slurm_job_id_monitor"
MONITOR_LAST_UPDATE_COLUMN = "last_update_monitor"

# use_shared_memory = sys.version_info >= (3, 8, 3)
# if use_shared_memory:
#     pass


def parse_args():
    parser = argparse.ArgumentParser(description="Train and/or validate models.")
    parser.add_argument("--csv_path", type=str, required=True, help="path to csv file")
    parser.add_argument(
        "--conda_env",
        type=str,
        required=False,
        default=DEFAULT_ENV_NAME,
        help="conda environment name",
    )
    parser.add_argument(
        "--run_locally", action="store_true", help="whether to run this script locally"
    )
    parser.add_argument(
        "--log_file_path",
        type=str,
        required=False,
        default=get_default_log_file_path(),
        help="default path for the log file",
    )
    parser.add_argument(
        "--expand",
        action="store_true",
        help="whether to first expand input csv by cartesian product of options",
    )

    parser.add_argument(
        "--use_socket",
        action="store_true",
        help="whether to use a socket for communication with the server",
    )

    parser.add_argument(
        "--disable_local_loging",
        action="store_true",
        help="whether to disable logging to wandb and g drive for local runs",
    )

    parser.add_argument(
        "--max_concurrent_jobs",
        type=int,
        required=False,
        default=-1,
        help="maximum number of concurrent jobs to run",
    )
    return parser.parse_args()

def get_default_log_file_path():
    return os.path.join(get_project_root_path(), "tmp", "tmp_log_for_run_from_csv.out")


def get_all_slurm_jobs():
    try:
        output = subprocess.check_output(["squeue", "--me"], universal_newlines=True, timeout=60)
        lines = output.strip().split("\n")[1:]  # Skip the header
        jobs = {}
        for line in lines:
            parts = line.split()
            job_id = int(parts[0])
            status = parts[4]
            jobs[job_id] = status
        return jobs
    except subprocess.CalledProcessError:
        return {}
    except subprocess.TimeoutExpired:
        return None


def get_pid_job_stats(jobs_ids, logger):
    job_stats_pid = {}
    for job_id in jobs_ids:
        # Just check if the process exists
        if process_exists(job_id):
            job_stats_pid[job_id] = JobStatus.RUNNING
        else:
            job_stats_pid[job_id] = JobStatus.FAILED  # I guess?
    return job_stats_pid


def get_slurm_jobs_stats(jobs_ids, logger):
    # Convert the list of job IDs to a comma-separated string
    job_ids_str = ",".join(map(str, jobs_ids))

    # Define the sacct command with the desired format
    cmd = ["sacct", "-j", job_ids_str, "--format=JobID,State,ExitCode", "--noheader"]

    # Execute the command and capture the output
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
    except subprocess.TimeoutExpired:
        logger.log("Warning: sacct command timed out! Will repeat after sleeping")
        return None
    # Split the output into lines and then into columns
    lines = result.stdout.strip().split("\n")
    job_stats = {}
    for line in lines:
        columns = line.split()
        # Check if the line contains the main job info (not the steps)
        if len(columns) == 3 and "+" not in columns[0]:
            job_id, state, exit_code = columns
            # check if str exists, extact ints only from job_id all ints not just sub
            job_id = int(re.sub("[^0-9]", "", job_id))
            job_stats[int(job_id)] = {"State": state, "ExitCode": exit_code}

    return job_stats


def update_job_statuses_in_place(
    job_manager: JobManager, shared_jobs_copy: Dict, logger, local_run=False
) -> bool:
    # Check SLURM cluster for running jobs and their status
    ref_key_job_id = "main_pid" if local_run else "job_id"
    job_ids = [job[ref_key_job_id] for job in shared_jobs_copy.values()]

    if not local_run:
        # Check SLURM cluster for jobs and their status
        all_jobs_stats = get_slurm_jobs_stats(job_ids, logger)
        if all_jobs_stats is None:
            return False

        if all_jobs_stats == {}:
            return True
    else:
        # Status should be updated in the shared_jobs_copy itself
        pass

    # updated_rows = []
    for row_id, job in shared_jobs_copy.items():
        if not local_run:
            job_stat = all_jobs_stats.get(job[ref_key_job_id], None)
            if job_stat:
                slurm_status = job_stat["State"]
                exit_code = job_stat["ExitCode"]
                job_status = get_slurm_job_status(slurm_status)
            else:
                # Job not found in SLURM - check if we have an explicit status
                if "status" in job:
                    job_status = job["status"]
                    exit_code = job.get("exit_code", None)
                else:
                    # No SLURM status and no explicit status
                    slurm_status = None
                    exit_code = None
                    job_status = None
        else:
            # For local jobs, use the status from shared dict
            job_status = job.get("status", None)
            exit_code = job.get("exit_code", None)

        job_from_local_idx = find_job_idx(job_manager.jobs, job[ref_key_job_id])
        if job_from_local_idx is None:
            # Add a job to the list and mark as update
            job_to_add = Job(
                job_id=job[ref_key_job_id],
                job_status=job_status,
                job_exit_code=exit_code,
                csv_row_id=row_id,
            )
            job_to_add.updated = True
            job_manager.jobs.append(job_to_add)
            # updated_rows.append(row_id)
        else:
            job_from_local = job_manager.jobs[job_from_local_idx]
            if job_from_local.csv_row_id is None:
                job_from_local.csv_row_id = row_id
            if job_from_local.job_status != job_status or job_from_local.job_exit_code != exit_code:
                # Update the job and mark as update
                job_from_local.job_status = job_status
                job_from_local.job_exit_code = exit_code
                job_from_local.updated = True
                # updated_rows.append(row_id)
            job_manager.jobs[job_from_local_idx] = job_from_local
    return True


def monitor_jobs_async(
    job_manager: JobManager,
    pool,
    job_submission_args,
    shared_jobs_dict,
    run_locally: bool,
    logger,
    spreadsheet_url,
    worksheet_name: str,
    shared_row_numbers,
    csv_path,
    gsheet_client: GspreadClient,
    lock,
    n_jobs_total,
    input_csv,
    max_conc_jobs=-1,
):
    # Initialize counters
    submitted = 0
    running = 0
    finished_successfully = 0
    failed = 0
    
    # Define a flag to check if we should exit
    should_exit = False
    signal_received = False  # Track if we've already received a signal
    is_cancelling = False   # Track if we're in the process of cancelling

    # Track which jobs are pending submission
    pending_jobs = list(range(len(job_submission_args)))
    active_submissions = {}  # job_idx -> AsyncResult
    submitted_not_running = set()  # Track jobs that are submitted but not yet running

    # Signal handler function
    def signal_handler(sig, frame):
        nonlocal should_exit, signal_received, is_cancelling
        if is_cancelling:
            # If we're already cancelling, ignore additional signals
            logger.log("Cancellation in progress, please wait...")
            return
        if not signal_received:
            signal_received = True
            run_jobs_flag.value = 0  # This will prevent new jobs from being submitted
            logger.log("Received exit signal. Preparing to terminate all jobs...")
            should_exit = True
        # If we get additional signals, just ignore them and let the cancellation complete

    # Register the signal handler
    original_sigint = signal.getsignal(signal.SIGINT)
    signal.signal(signal.SIGINT, signal_handler)

    try:
        gsheet_updater = None
        shared_row_numbers_lst = list(shared_row_numbers)

        if csv_path is None:
            assert run_locally, "If not running locally, a csv file must be provided."
            warn("No csv file provided, skipping writing to a csv file.")
        else:
            gsheet_updater = GSheetBatchUpdater(
                spreadsheet_url, worksheet_name, gsheet_client, logger, csv_path, input_csv
            )
            # get cols from gsheet_client using the `worksheet_name`
            worksheet = gsheet_client.opened_spreadsheet.worksheet(worksheet_name)
            # Get the first row (headers)
            headers_org = worksheet.row_values(1)

            # Prepare the updates
            cols_not_reset = ["path_to_main", "path_to_default_config", "custom_run_cmd"]

            # also exclude slurm and delta columns
            cols_not_reset += [
                col
                for col in headers_org
                if col.startswith(SLURM_PREFIX) or col.startswith(DELTA_PREFIX)
            ]

            headers_reset = [col for col in headers_org if col not in cols_not_reset]

            column_value_pairs = [
                (MONITOR_LAST_UPDATE_COLUMN, datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
                (MONITOR_STATUS_COLUMN, SUBMITTED_STATUS),
                (WHETHER_TO_RUN_COLUMN, "0"),
            ]
            for column_name in headers_reset:
                if column_name != WHETHER_TO_RUN_COLUMN:
                    column_value_pairs.append((column_name, ""))

            # Add updates to the queue
            for row_id_share in shared_row_numbers_lst:
                for key, val in column_value_pairs:
                    gsheet_updater.add_to_queue(row_id_share, key, val)

            # Write updates to CSV and then batch update the Google Sheet
            update_status = gsheet_updater.batch_update()
            logger.log(f"Update status to GSheets: {update_status}")

        total_jobs = n_jobs_total
        if total_jobs == 0:
            logger.log("No jobs submitted, exiting.")
            return

        current_sleep_duration = 1
        sleep_increment = 1
        max_sleep_duration = 15

        while not should_exit:
            # Check current running jobs and pending submissions
            current_jobs = 0
            if not run_locally:
                try:
                    # Get all jobs and their statuses
                    active_job_ids = set()
                    with lock:
                        shared_jobs_copy = dict(shared_jobs_dict)
                        for job_info in shared_jobs_copy.values():
                            if 'job_id' in job_info and 'status' in job_info:
                                status = job_info['status']
                                # Only count jobs that are actually running or pending
                                if status in [JobStatus.RUNNING, JobStatus.PENDING]:
                                    active_job_ids.add(job_info['job_id'])
                                    
                    # Also check SLURM queue to verify jobs are actually running
                    try:
                        output = subprocess.check_output(["squeue", "--me", "--noheader"], universal_newlines=True)
                        slurm_jobs = set(line.split()[0] for line in output.strip().split('\n') if line.strip())
                        # Only keep jobs that are both in our tracking AND in SLURM queue
                        active_job_ids = {str(job_id) for job_id in active_job_ids if str(job_id) in slurm_jobs}
                    except subprocess.CalledProcessError:
                        pass  # If squeue fails, fall back to our tracking
                    
                    # Count only jobs that are verified to be running
                    current_jobs = len(active_job_ids) + len(active_submissions)
                    logger.log(f"DEBUG: Found {current_jobs} active jobs (verified in SLURM)")
                except Exception as e:
                    logger.log(f"Failed to check job status: {str(e)}")
                    time.sleep(5)
                    continue
            else:
                current_jobs = running_jobs.value

            # Submit new jobs if under limit
            logger.log(f"DEBUG: Before submission - current_jobs: {current_jobs}, active_submissions: {len(active_submissions)}, max_conc_jobs: {max_conc_jobs}")
            while pending_jobs and (max_conc_jobs == -1 or current_jobs < max_conc_jobs):
                job_idx = pending_jobs.pop(0)
                args_submit = job_submission_args[job_idx]
                logger.log(f"DEBUG: Submitting job {job_idx}")
                active_submissions[job_idx] = pool.apply_async(submit_job, args_submit)
                current_jobs += 1
                logger.log(f"Submitting job {job_idx}, current jobs: {current_jobs}/{max_conc_jobs}")

            # Check completed submissions
            completed = []
            for job_idx, async_result in active_submissions.items():
                if async_result.ready():
                    try:
                        result = async_result.get(timeout=1)
                        logger.log(f"DEBUG: Job {job_idx} submission completed with result {result}")
                        if result != 0:
                            logger.log(f"Job submission failed for index {job_idx}")
                            current_jobs -= 1  # Decrement since submission failed
                            pending_jobs.append(job_idx)
                    except Exception as e:
                        logger.log(f"Error checking job {job_idx}: {str(e)}")
                        current_jobs -= 1  # Decrement since submission failed
                        pending_jobs.append(job_idx)
                    completed.append(job_idx)

            # Remove completed submissions
            for job_idx in completed:
                del active_submissions[job_idx]

            # Wait for job manager to register the jobs and update statuses
            if not run_locally:
                with job_manager.manager_lock:
                    jobs_update_status = update_job_statuses_in_place(
                        job_manager, shared_jobs_dict, logger, run_locally
                    )
                    if jobs_update_status and gsheet_updater is not None:
                        dump_into_gsheet_queue(gsheet_updater, job_manager)

            # Count the statuses
            submitted = submitted_jobs.value
            running = sum(1 for job in job_manager.jobs if job.job_status == JobStatus.RUNNING)
            finished_successfully = sum(1 for job in job_manager.jobs if job.job_status == JobStatus.COMPLETED)
            failed = sum(
                1
                for job in job_manager.jobs
                if job.job_status in [JobStatus.FAILED, JobStatus.CANCELLED, JobStatus.TIMEOUT]
            )

            # Display progress and status
            last_update_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            status_msg = (
                f"Jobs Progress | Submitted: {submitted}/{total_jobs} | "
                f"Running: {running}/{total_jobs} | "
                f"Finished: {finished_successfully}/{total_jobs} | "
                f"Failed: {failed} | "
                f"Pending: {len(pending_jobs)} | "
                f"Active Submissions: {len(active_submissions)} | "
                f"Last Update: {last_update_time}"
            )
            logger.log(status_msg, carriage_return=True)

            if gsheet_updater is not None:
                gsheet_updater.batch_update()

            time.sleep(current_sleep_duration)
            current_sleep_duration = min(current_sleep_duration + sleep_increment, max_sleep_duration)

            if finished_successfully + failed == total_jobs:
                logger.log("All jobs finished.")
                break

    finally:
        if should_exit:
            is_cancelling = True  # Mark that we're in cancellation process
            # Save original handlers
            original_sigint = signal.getsignal(signal.SIGINT)
            original_sigterm = signal.getsignal(signal.SIGTERM)
            
            # Block all signals during critical operations
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            signal.signal(signal.SIGTERM, signal.SIG_IGN)
            
            try:
                logger.log("Cancelling pending submissions...")
                for job_idx in list(active_submissions.keys()):
                    async_result = active_submissions[job_idx]
                    if not async_result.ready():
                        # Mark as cancelled in shared dict
                        with lock:
                            row_id = job_submission_args[job_idx][-3]  # Get row_id from args
                            if row_id in shared_jobs_dict:
                                job_info = shared_jobs_dict[row_id].copy()
                                job_info['status'] = JobStatus.CANCELLED
                                shared_jobs_dict[row_id] = job_info
                
                # Update status for pending jobs before proceeding
                with lock:
                    shared_jobs_copy = dict(shared_jobs_dict)
                with job_manager.manager_lock:
                    jobs_update_status = update_job_statuses_in_place(
                        job_manager, shared_jobs_copy, logger, run_locally
                    )
                    if jobs_update_status and gsheet_updater is not None:
                        dump_into_gsheet_queue(gsheet_updater, job_manager)
                        gsheet_updater.batch_update(force=True)

                logger.log("Cancelling running jobs...")
                running_job_ids = []
                with lock:
                    shared_jobs_copy = dict(shared_jobs_dict)
                    for row_id, job_info in shared_jobs_copy.items():
                        if 'job_id' in job_info:
                            running_job_ids.append((job_info['job_id'], row_id))
                            # Update status in shared dict
                            job_info['status'] = JobStatus.CANCELLED
                            shared_jobs_dict[row_id] = job_info
                        elif 'main_pid' in job_info:
                            running_job_ids.append((job_info['main_pid'], row_id))
                            # Update status in shared dict
                            job_info['status'] = JobStatus.CANCELLED
                            shared_jobs_dict[row_id] = job_info

                if running_job_ids:
                    logger.log(f"Sending cancellation to {len(running_job_ids)} jobs...")
                    # Split into SLURM and local jobs
                    slurm_jobs = []
                    local_jobs = []
                    for job_id, row_id in running_job_ids:
                        if run_locally:
                            local_jobs.append(job_id)
                        else:
                            slurm_jobs.append(str(job_id))

                    # Cancel all SLURM jobs at once
                    if slurm_jobs:
                        logger.log("Cancelling SLURM jobs...")
                        max_retries = 3
                        for retry in range(max_retries):
                            try:
                                # First try normal cancel
                                subprocess.run(["scancel", *slurm_jobs], check=True)
                                
                                # Wait and verify jobs are gone
                                remaining_jobs = []
                                for _ in range(5):  # Check multiple times with delay
                                    remaining_jobs = []
                                    try:
                                        output = subprocess.check_output(["squeue", "--me", "--noheader"], universal_newlines=True)
                                        for line in output.strip().split('\n'):
                                            if line.strip():
                                                job_id = line.split()[0]
                                                if job_id in slurm_jobs:
                                                    remaining_jobs.append(job_id)
                                    except subprocess.CalledProcessError:
                                        break  # No jobs in queue is good
                                    
                                    if not remaining_jobs:
                                        break
                                    logger.log(f"Waiting for {len(remaining_jobs)} jobs to cancel...")
                                    time.sleep(2)
                                
                                if not remaining_jobs:
                                    logger.log("All SLURM jobs cancelled successfully")
                                    break
                                
                                # If jobs still exist, try force kill
                                if retry < max_retries - 1:
                                    logger.log(f"Some jobs still running, trying force kill (attempt {retry + 1}/{max_retries})...")
                                    subprocess.run(["scancel", "--signal=KILL", *remaining_jobs], check=True)
                                else:
                                    logger.log(f"WARNING: Failed to cancel jobs after {max_retries} attempts: {remaining_jobs}")
                                    
                            except Exception as e:
                                logger.log(f"Error during cancellation attempt {retry + 1}: {str(e)}")
                                if retry == max_retries - 1:
                                    logger.log("Failed to cancel all jobs, continuing with cleanup...")

                    # Kill local jobs one by one (we have to since they're processes)
                    for job_id in local_jobs:
                        if process_exists(job_id):
                            try:
                                os.kill(job_id, signal.SIGKILL)
                            except:
                                pass

                # Make sure all jobs are marked as cancelled in job manager
                logger.log("Updating final status in sheets...")
                try:
                    with lock:
                        shared_jobs_copy = dict(shared_jobs_dict)
                    with job_manager.manager_lock:
                        # First ensure all jobs are marked as cancelled in job manager
                        for job in job_manager.jobs:
                            if job.job_status in [JobStatus.RUNNING, JobStatus.PENDING]:
                                job.job_status = JobStatus.CANCELLED
                                job.updated = True
                                # Also update in shared dict to ensure consistency
                                if job.csv_row_id in shared_jobs_dict:
                                    job_info = shared_jobs_dict[job.csv_row_id].copy()
                                    job_info['status'] = JobStatus.CANCELLED
                                    shared_jobs_dict[job.csv_row_id] = job_info
                        
                        # Force a final status update
                        jobs_update_status = update_job_statuses_in_place(
                            job_manager, shared_jobs_copy, logger, run_locally
                        )

                        # Make sure all cancelled jobs are marked for update
                        for job in job_manager.jobs:
                            if job.job_status == JobStatus.CANCELLED:
                                job.updated = True

                        if gsheet_updater is not None:
                            logger.log("Pushing final status to sheets...")
                            # First update
                            dump_into_gsheet_queue(gsheet_updater, job_manager)
                            gsheet_updater.batch_update(force=True)
                            
                            # Double check all cancelled jobs are in the update queue
                            for job in job_manager.jobs:
                                if job.job_status == JobStatus.CANCELLED:
                                    gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_STATUS_COLUMN, "CANCELLED")
                                    gsheet_updater.add_to_queue(
                                        job.csv_row_id,
                                        MONITOR_LAST_UPDATE_COLUMN,
                                        datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                    )
                            
                            # Final update
                            logger.log("Waiting for sheet update to complete...")
                            gsheet_updater.batch_update(force=True)
                            time.sleep(2)  # Give time for update to propagate
                            logger.log("Sheet update completed.")

                except Exception as e:
                    logger.log(f"Failed to update sheets: {str(e)}")
                    logger.log("WARNING: Sheet updates may not have completed!")
                    time.sleep(5)  # Give a moment for the error to be visible
                    raise  # Re-raise to ensure we don't silently fail

                logger.log("All jobs cancelled and sheets updated.")
            except Exception as e:
                logger.log(f"Error during cancellation: {str(e)}")
                raise
            finally:
                # Only unset flags and exit after everything is truly done
                is_cancelling = False
                signal.signal(signal.SIGINT, original_sigint)
                signal.signal(signal.SIGTERM, original_sigterm)
                sys.exit(1)


def dump_into_gsheet_queue(gsheet_updater, job_manager: JobManager):
    for idx, job in enumerate(job_manager.jobs):
        if job.updated:
            gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_STATUS_COLUMN, job.job_status)
            gsheet_updater.add_to_queue(
                job.csv_row_id,
                MONITOR_LAST_UPDATE_COLUMN,
                datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            )
            if job.job_exit_code is not None:
                gsheet_updater.add_to_queue(
                    job.csv_row_id, MONITOR_EXIT_CODE_COLUMN, f"{job.job_exit_code}"
                )
            if job.job_id is not None:
                gsheet_updater.add_to_queue(job.csv_row_id, MONITOR_JOB_ID_COLUMN, f"{job.job_id}")

            # check the queue of each job too
            for q_item in job.writing_queue:
                gsheet_updater.add_to_queue(job.csv_row_id, q_item[0], q_item[1])
            # job_manager.jobs[idx].writing_queue = []
            # job_manager.jobs[idx].updated = False
            job.writing_queue = []
            job.updated = False

            job_manager.jobs[idx] = job


def main_with_monitoring(
    make_final_cmd=None, allowed_prefixes=(SLURM_PREFIX, DELTA_PREFIX, HTTP_PREFIX)
):
    # Determine to which region we belong
    cluster_region = get_region()
    if cluster_region == CLUSTER.UNKNOWN:
        print("Unknown cluster region. Using partitions are specified")
    else:
        print(f"We're on cluster region: {cluster_region}, using only the corresponding partitions")

    if make_final_cmd is None:
        make_final_cmd = make_final_cmd_slurm

    args = parse_args()
    max_conc_jobs = -1 if "max_concurrent_jobs" not in args else args.max_concurrent_jobs
    use_socket = args.use_socket if args.use_socket is not None else False
    disable_local_loging = args.disable_local_loging if args.disable_local_loging is not None else False

    logger = make_logger()

    if use_socket:
        logger.log("User requested to use socket.")

    job_manager = JobManager(
        local_run=args.run_locally if args.run_locally is not None else False,
        open_socket=use_socket,
        logger=logger,
    )

    # Make sure server ip and port are set
    if job_manager.open_socket:
        logger.log("Trying to get server ip and port")
        while job_manager.server_ip is None or job_manager.server_port is None:
            time.sleep(1)

    csv_path_or_url = args.csv_path
    logger.log(f"Fetching csv from: {csv_path_or_url}")
    csv_path, spreadsheet_url, worksheet_name, gspread_client = fetch_csv(csv_path_or_url, logger)

    if args.expand:
        expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client)

    inputs_csv = read_csv_as_dict(csv_path)

    with mp.Manager() as shared_memory_manager:
        lock = shared_memory_manager.Lock()
        current_step = shared_memory_manager.Value("int", 0)
        shared_rows_to_run = shared_memory_manager.list()
        shared_csv_updates = shared_memory_manager.list()
        shared_default_config_paths = shared_memory_manager.dict()
        shared_row_numbers = shared_memory_manager.list()
        shared_jobs_dict = shared_memory_manager.dict()

        # First process all rows to prepare commands
        starmap_args_for_row_processing = [
            (
                make_final_cmd,
                csv_row,
                row_number,
                csv_path,
                args.conda_env,
                args.run_locally,
                args.log_file_path,
                spreadsheet_url,
                worksheet_name,
                logger,
                lock,
                shared_rows_to_run,
                shared_default_config_paths,
                shared_csv_updates,
                shared_row_numbers,
                current_step,
                len(inputs_csv),
                job_manager.server_ip,
                job_manager.server_port,
                disable_local_loging,
                cluster_region,
            )
            for row_number, csv_row in inputs_csv.items()
        ]

        if len(starmap_args_for_row_processing):
            pool_size = get_pool_size(len(starmap_args_for_row_processing))
            with mp.Pool(pool_size) as pool:
                # Process all rows first to prepare commands
                pool.starmap(process_csv_row, starmap_args_for_row_processing)

                if len(shared_rows_to_run):
                    assert 2 * len(shared_rows_to_run) == len(shared_csv_updates)
                    concurrent_log_func = retrier_factory()(log_csv_for_concurrent)
                    concurrent_log_func(csv_path, shared_csv_updates, use_socket=True)
                    os.makedirs(os.path.dirname(args.log_file_path), exist_ok=True)

                    # Prepare job submission arguments
                    job_submission_args = [
                        (
                            run_cmd,
                            args.log_file_path,
                            args.run_locally,
                            shared_jobs_dict,
                            row_id,
                            lock,
                            logger,
                            max_conc_jobs,
                        )
                        for run_cmd, row_id in zip(shared_rows_to_run, shared_row_numbers)
                    ]

                    # Submit jobs in batches and monitor them
                    monitor_jobs_async(
                        job_manager,
                        pool,
                        job_submission_args,
                        shared_jobs_dict,
                        args.run_locally,
                        logger,
                        spreadsheet_url,
                        worksheet_name,
                        shared_row_numbers,
                        csv_path,
                        gspread_client,
                        lock,
                        len(job_submission_args),
                        inputs_csv,
                        max_conc_jobs,
                    )

    # Clean up server
    if job_manager.open_socket:
        try:
            logger.log("Attempting to stop the server...")
            if job_manager.server_process and job_manager.server_process.is_alive():
                job_manager.server_process.terminate()
                job_manager.server_process.join(timeout=5)
                if job_manager.server_process.is_alive():
                    logger.log("Server didn't terminate gracefully, forcing...")
                    job_manager.server_process.kill()
            logger.log("Server stopped successfully")
        except Exception as e:
            logger.log(f"Error while stopping server: {str(e)}")

    logger.log("Job done, exiting.")
    os._exit(0)


def get_pool_size(iterable_len):
    return min(min(max(1, mp.cpu_count() - 1), iterable_len), MAX_PROCESSES)


# TODO: move these into the main function
run_jobs_flag = mp.Value("i", 1)  # 1 means jobs should run, 0 means they shouldn't
submitted_jobs = mp.Value("i", 0)
running_jobs = mp.Value("i", 0)
submitted_jobs_lock = mp.Lock()


def update_job_status(process, shared_jobs_dict, row_id, lock_manager, cancelled=False):
    """Update the job status and exit code in the shared dictionary."""
    with lock_manager:
        # print(f"Before update: {shared_jobs_dict[row_id]}")
        job_data = shared_jobs_dict[row_id].copy()  # Get a local copy
        if cancelled:
            job_data["status"] = JobStatus.CANCELLED
        else:
            if process.returncode == 0:
                job_data["status"] = JobStatus.COMPLETED
            else:
                job_data["status"] = JobStatus.FAILED
        job_data["exit_code"] = process.returncode
        shared_jobs_dict[row_id] = job_data  # Set the modified data back
        # print(f"After update: {shared_jobs_dict[row_id]}")


def submit_job(
    run_cmd,
    log_file_path,
    run_locally,
    shared_jobs_dict,
    row_id,
    lock_manager,
    logger,
    max_conc_jobs=-1,
):
    """
    row_id is used for identifying the job in the spreadsheet and for updating the status of the job in the shared memory.
    """

    if not run_jobs_flag.value:
        return "Job Stopped"

    with open(log_file_path, "w+") as log_file:
        if run_locally:
            def get_main_and_child_pids(pid):
                main_pid = pid
                child_pids = [child.pid for child in psutil.Process(main_pid).children()]
                return main_pid, child_pids

            while max_conc_jobs != -1 and running_jobs.value >= max_conc_jobs:
                if not run_jobs_flag.value:
                    return "Job Stopped"
                time.sleep(1)

            running_jobs.value += 1
            split_command = shlex.split(run_cmd)
            process = subprocess.Popen(split_command, stdout=log_file, stderr=log_file, shell=False)

            with submitted_jobs_lock:
                submitted_jobs.value += 1

            main_pid, child_pids = get_main_and_child_pids(process.pid)
            try:
                with lock_manager:
                    shared_jobs_dict[row_id] = {"main_pid": main_pid, "status": JobStatus.RUNNING}
            except Exception as e:
                pass
            # Periodically check if the process is still running
            while process.poll() is None:
                # Check the run_jobs_flag during execution
                if not run_jobs_flag.value:
                    logger.log(f"Stopping job with row ID: {row_id}")
                    # Send a SIGINT signal for graceful exit
                    process.send_signal(signal.SIGINT)

                    # Wait for up to 5 seconds for the process to exit gracefully
                    try:
                        process.wait(timeout=2)
                    except subprocess.TimeoutExpired:
                        # If the process doesn't exit within 5 seconds, terminate it forcefully
                        process.terminate()

                    # Update the job status after forcefully terminating
                    update_job_status(
                        process, shared_jobs_dict, row_id, lock_manager, cancelled=True
                    )
                    running_jobs.value -= 1
                    return 1

                time.sleep(1)  # Sleep for a short duration before checking again

            # Process finished
            update_job_status(process, shared_jobs_dict, row_id, lock_manager)
            running_jobs.value -= 1
            return process.returncode
        else:
            timeout_duration = 60

            try:
                if not run_jobs_flag.value:
                    return "Job Stopped"
                    
                output = subprocess.check_output(
                    run_cmd, stderr=subprocess.STDOUT, shell=True, timeout=timeout_duration
                ).decode("utf-8")
                numbers = re.findall(r"\d+", output)
                job_id = int("".join(numbers))

                if job_id:
                    logger.log(f"Submitted a job with ID: {job_id}")
                else:
                    logger.log("Failed to extract job ID from the output.")

            except subprocess.CalledProcessError as e:
                print(f"Command failed with error: {e.output.decode('utf-8')}")
                return 1
            except subprocess.TimeoutExpired:
                print(f"The command took longer than {timeout_duration} seconds to complete.")
                return 1

            with submitted_jobs_lock:
                submitted_jobs.value += 1

            with lock_manager:
                shared_jobs_dict[row_id] = {
                    "job_id": job_id,
                    "status": JobStatus.PENDING,  # Changed from "submitted" to use JobStatus enum
                }

            return 0


def expand_gsheet(csv_path, spreadsheet_url, worksheet_name, gspread_client):
    expanded_csv_path = os.path.join(
        os.path.dirname(csv_path), EXPANDED_CSV_PREFIX + os.path.basename(csv_path)
    )
    expand_csv(csv_path, expanded_csv_path)
    csv_path = expanded_csv_path
    if worksheet_name is not None:
        worksheet_name = EXPANDED_CSV_PREFIX + worksheet_name

    try_to_upload_csv(csv_path, spreadsheet_url, worksheet_name, gspread_client)


def fetch_default_config_path(path, logger):
    if FILES_URL in path:
        gdrive_client = make_gdrive_client(logger)
        with NamedTemporaryFile("w+t", delete=False) as tmp_file:
            remote_file = sync_local_file_with_gdrive(
                gdrive_client, tmp_file.name, path, download=True, logger=logger
            )

            file_path = os.path.join(
                get_default_configs_folder(),
                remote_file["title"].split(".")[0],
                f"default_config.yaml",
            )

            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            shutil.move(tmp_file.name, file_path)

        return file_path

    else:
        try:
            return normalize_path(path)
        except Exception as e:
            logger.log(f"Couldn't fetch default config path at {path}!")


def get_default_configs_folder():
    return os.path.join(get_project_root_path(), "experiment_configs")


def process_csv_row(
    make_final_cmd,
    csv_row,
    row_number,
    input_csv_path,
    conda_env,
    run_locally,
    log_file_path,
    spreadsheet_url,
    worksheet_name,
    logger,
    lock,
    shared_rows_to_run,
    shared_default_config_paths,
    shared_csv_updates,
    shared_row_numbers,
    current_step,
    total_rows,
    server_ip,
    server_port,
    disable_local_loging,
    cluster_region,
):
    assert not spreadsheet_url or worksheet_name is not None, (
        "`worksheet_name` is None but this is not allowed when remote sheet is used;"
        "Make sure to pass the worksheet name using the `::` syntax in the --csv_path argument."
    )
    default_config_path = ""
    final_cmd = None

    whether_to_run = csv_row[WHETHER_TO_RUN_COLUMN]

    gpu_fits_cluster = True
    # check if the partition corresponds to login node
    if "slurm:partition" in csv_row:
        try:
            partition = csv_row["slurm:partition"]
        except Exception as e:
            logger.log(f"Skipping row {row_number} - 'slurm:pa  rtition' not found in csv_row")
            logger.log(f"Error: {e}")
            return
        # using galvani's gpus or ferranti's
        try:
            if "h100" in partition and cluster_region == CLUSTER.GALVANI:
                gpu_fits_cluster = False
                logger.log(
                    f"Skipping row {row_number} - H100 job not supported on Galvani cluster"
                )
                return
            if ("a100" in partition and cluster_region == CLUSTER.FERRANTI) or ("2080" in partition and cluster_region == CLUSTER.FERRANTI):
                gpu_fits_cluster = False
                logger.log(
                    f"Skipping row {row_number} - {partition} job not supported on Ferranti cluster"
                )
                return
        except Exception as e:
            logger.log(f"Skipping row {row_number} - 'slurm:partition' not found in csv_row")
            logger.log(f"Error: {e}")
            return
            
    try:
        if gpu_fits_cluster and whether_to_run.isnumeric() and int(whether_to_run) != 0:
            replace_placeholders(csv_row, CURRENT_ROW_PLACEHOLDER, str(row_number))
            replace_placeholders(csv_row, CURRENT_WORKSHEET_PLACEHOLDER, worksheet_name)

            default_config_path_or_url = csv_row[PATH_TO_DEFAULT_CONFIG_COLUMN]
            if not default_config_path_or_url in shared_default_config_paths:
                with lock:
                    try:
                        default_config_path = fetch_default_config_path(
                            default_config_path_or_url, logger
                        )
                    except Exception as e:
                        # log also row number etc
                        logger.log(
                            f"Failed to fetch default config path at {default_config_path_or_url}."
                            f"\nIt occurs in row {row_number} of {input_csv_path}."
                        )
                        raise e
                    shared_default_config_paths[default_config_path_or_url] = default_config_path
            else:
                default_config_path = shared_default_config_paths[default_config_path_or_url]
            if default_config_path is None:
                error_msg = f"Default config path at {default_config_path} is None. (occurs in row {row_number} of {input_csv_path})"
                logger.log(error_msg)
                raise ValueError(error_msg)
            if not os.path.exists(default_config_path):
                error_msg = f"Default config path at {default_config_path} does not exist (occurs in row {row_number} of {input_csv_path})"
                logger.log(error_msg)
                raise FileNotFoundError(error_msg)

            exp_dir = normalize_path(os.path.dirname(default_config_path))
            exp_name = os.path.basename(exp_dir)

            default_config = read_yaml(default_config_path)

            if disable_local_loging:
                if "logging" in default_config and isinstance(default_config["logging"], dict):
                    default_config["logging"]["use_wandb"] = False
                    default_config["logging"]["gdrive_storage_folder"] = None

            _, new_config_path = make_new_config(
                csv_row,
                row_number,
                input_csv_path,
                default_config,
                exp_dir,
                spreadsheet_url,
                worksheet_name,
                server_ip,
                server_port,
                run_locally,
            )

            cmd_as_string = make_task_cmd(
                new_config_path,
                conda_env,
                normalize_path(csv_row[MAIN_PATH_COLUMN]),
                csv_row,  # pass the row since might need to overwrite running command
            )

            log_folder = os.path.dirname(log_file_path)
            if not os.path.exists(log_folder):
                with lock:
                    os.makedirs(log_folder, exist_ok=True)

            if run_locally:
                final_cmd = "{}".format(cmd_as_string)
            else:
                final_cmd = make_final_cmd(csv_row, exp_name, log_file_path, cmd_as_string)

        if final_cmd is not None:
            shared_csv_updates.append((row_number, STATUS_CSV_COLUMN, SUBMITTED_STATUS))
            shared_csv_updates.append((row_number, WHETHER_TO_RUN_COLUMN, "0"))
            shared_rows_to_run.append(final_cmd)
            shared_row_numbers.append(row_number)

        with lock:
            current_step.value += 1
            logger.progress("Rows processing.", current_step.value, total_rows)
            
    except Exception as e:
        logger.log(f"Error processing row {row_number}: {str(e)}\nTraceback: {traceback.format_exc()}")
        raise  # Re-raise the exception after logging


def make_final_cmd_slurm(csv_row, exp_name, log_file_path, cmd_as_string):
    slurm_args_dict = make_slurm_args_dict(csv_row, exp_name, log_file_path)
    if USE_SRUN:
        slurm_args_as_string = " ".join(
            [f"--{flag}={value}" for flag, value in slurm_args_dict.items()]
        )
        final_cmd = 'srun {} sh -c "{}" &'.format(slurm_args_as_string, cmd_as_string)
    else:
        with NamedTemporaryFile("w", delete=False) as tmp_file:
            fill_sbatch_script(tmp_file, slurm_args_dict, cmd_as_string)
            final_cmd = "sbatch {}".format(tmp_file.name)

    return final_cmd


def check_csv_column_names(csv_row, allowed_prefixes):
    assert MAIN_PATH_COLUMN in csv_row

    assert WHETHER_TO_RUN_COLUMN in csv_row

    assert PATH_TO_DEFAULT_CONFIG_COLUMN in csv_row

    for i, key in enumerate(csv_row.keys()):
        assert key is not None, (
            f"Column {i} has empty column name. " f"Or some table entries contain commas."
        )
        if PREFIX_SEPARATOR in key:
            assert any([prefix in key for prefix in allowed_prefixes]), (
                f'"{key}" does not contain any of allowed prefixes ' f"from:\n{allowed_prefixes}\n"
            )


def fill_sbatch_script(sbatch_file, slurm_args_dict, command):
    sbatch_file.write("#!/bin/bash\n")

    for slurm_arg, value in slurm_args_dict.items():
        sbatch_file.write("#SBATCH --{}={}\n".format(slurm_arg, value))

    sbatch_file.write(command)
    sbatch_file.flush()


def make_new_config(
    csv_row,
    row_number,
    input_csv_path,
    default_config,
    exp_dir,
    spreadsheet_url,
    worksheet_name,
    server_ip,
    server_port,
    run_locally,
):
    """
    Assume that `fixed_params` are the ones being passed to the jobs. We will move them to the config file
    but will update them according to the `deltas` in the config file.
    """
    deltas = extract_from_csv_row_by_prefix(
        csv_row, DELTA_PREFIX + PREFIX_SEPARATOR, ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        """Make changes only in the 'fixed_params' subdict of the config"""
        assert (
            "fixed_params" in default_config
        ), "If DELTA_AFFECTS_ONLY_FIXED_PARAMS is True, then 'fixed_params' must be in default_config."
        for key in list(deltas.keys()):
            deltas["fixed_params" + NESTED_CONFIG_KEY_SEPARATOR + key] = deltas[key]
            del deltas[key]

    if len(deltas) > 0:
        check_duplicates(list(deltas.keys()))

    deltas_del = []
    for key in deltas.keys():
        value = deltas[key]
        if value == EMPTY_STRING:
            deltas[key] = ""
        elif value == "":
            if EMPTY_VALUE_MEANS_NO_CHANGE:
                # erase the key
                deltas_del.append(key)
                warnings.warn("WARNING: Empty value for {} will be ignored.".format(key))
            else:
                raise Exception(f"Empty value for {make_delta_column_name(key)}")
    for key in deltas_del:
        del deltas[key]

    decode_strings_in_dict(
        deltas, list_separators=[" "], list_start_symbol="[", list_end_symbol="]"
    )

    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}output_csv"] = make_csv_config(
        input_csv_path, row_number, spreadsheet_url, worksheet_name
    )

    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}server_ip"] = server_ip
    deltas[f"logging{NESTED_CONFIG_KEY_SEPARATOR}server_port"] = server_port

    deltas["run_locally"] = run_locally

    new_config = make_config_from_default_and_deltas(default_config, deltas)
    # make sure we preserve deltas though
    for delta in deltas:
        if delta == f"logging{NESTED_CONFIG_KEY_SEPARATOR}output_csv":
            continue
        new_config[DELTA_PREFIX + PREFIX_SEPARATOR + delta] = deltas[delta]

    if DELTA_AFFECTS_ONLY_FIXED_PARAMS:
        # Copy stuff from `fixed_params` to the root of the config
        for key, value in new_config["fixed_params"].items():
            new_config[key] = value

    new_config_path = os.path.join(
        exp_dir, AUTOGEN_PREFIX, make_autogenerated_config_name(input_csv_path, row_number)
    )
    os.makedirs(os.path.dirname(new_config_path), exist_ok=True)
    save_as_yaml(new_config_path, new_config)
    return new_config, new_config_path


def replace_placeholders(csv_row, placeholder, new_value):
    for column_name, value in csv_row.items():
        csv_row[column_name] = str(value).replace(placeholder, new_value)


def make_task_cmd(new_config_path, conda_env, exec_path, csv_row):
    exec_args = "--config_path {}".format(new_config_path)

    # If `custom_run_cmd` is passed, overwrite the default command
    # with the custom one.
    if "custom_run_cmd" in csv_row:
        cmd = "{} {} {}".format(csv_row["custom_run_cmd"], exec_path, exec_args)
    else:
        cmd = "{} {} && python {} {}".format(
            NEW_SHELL_INIT_COMMAND, conda_env, exec_path, exec_args
        )
    return cmd


def make_slurm_args_dict(csv_row, exp_name, log_file):
    all_slurm_args_dict = copy.deepcopy(DEFAULT_SLURM_ARGS_DICT)

    all_slurm_args_dict["job-name"] = exp_name

    all_slurm_args_dict["output"] = log_file
    all_slurm_args_dict["error"] = log_file

    specified_slurm_args = extract_from_csv_row_by_prefix(
        csv_row, SLURM_PREFIX + PREFIX_SEPARATOR, ignore_values=PLACEHOLDERS_FOR_DEFAULT
    )

    all_slurm_args_dict |= specified_slurm_args

    os.makedirs(os.path.dirname(all_slurm_args_dict["output"]), exist_ok=True)
    os.makedirs(os.path.dirname(all_slurm_args_dict["error"]), exist_ok=True)

    return all_slurm_args_dict


def extract_from_csv_row_by_prefix(csv_row, prefix, ignore_values):
    prefix_len = len(prefix)
    result = {}
    for key, value in csv_row.items():
        assert (
            key is not None
        ), f"Possibly inconsistent number of delimeters. Found key={key} in csv_row={csv_row} with value={value}"
        if key == prefix:
            raise Exception(
                f'Found "{prefix}" (nothing after this prefix) ' f"in csv_row:\n{csv_row}"
            )
        if len(key) > prefix_len and prefix == key[:prefix_len] and not value in ignore_values:
            result[key[prefix_len:]] = value

    return result


def make_config_from_default_and_deltas(default_config, deltas):
    assert isinstance(deltas, dict)
    new_config = copy.deepcopy(default_config)
    for nested_config_key, new_value in deltas.items():
        update_dict_by_nested_key(
            new_config,
            nested_config_key.split(NESTED_CONFIG_KEY_SEPARATOR),
            new_value,
            to_create_new_elements=True,
        )
    return new_config


if __name__ == "__main__":
    main_with_monitoring()
